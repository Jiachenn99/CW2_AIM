{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "celltoolbar": "Attachments",
    "colab": {
      "name": "CW2 - AIM",
      "provenance": [],
      "collapsed_sections": [
        "kgzFFe0fWjGL",
        "4Jqi0TGLXNLi",
        "WHRtBqQPXUgZ",
        "9pz5Mgv6X3mT",
        "O6Jaw59JYa8Z",
        "P8OMy3wF1IIl",
        "DaRvVV5mDddf",
        "HRaTOxAyyXq2",
        "9T7bv9bDEiXo",
        "Z7JIg_V2E3t5",
        "PlUQsx5tFYB_",
        "TXbK2v51xMpH",
        "JqEVks-MFrRP",
        "TRsXu3X1toBF",
        "vRtllDvYG9wi",
        "kBTp6w0Nzd_X",
        "FCl2YedlFrAM"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jiachenn99/CW2_AIM/blob/offspring_creation/CW2_AIM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kgzFFe0fWjGL"
      },
      "source": [
        "# Imports, Initialization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8o89uxX4xG1G",
        "outputId": "91f08eed-8929-40a2-f114-97178d0fc9d1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#@title\n",
        "from __future__ import print_function\n",
        "version = '_v0p3p9_'\n",
        "\n",
        "import time\n",
        "import warnings\n",
        "import csv\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.mlab as mlab\n",
        "import math\n",
        "import scipy\n",
        "import pytz \n",
        "import random\n",
        "\n",
        "from datetime import datetime, timezone,timedelta\n",
        "from numpy import asarray\n",
        "from numpy import savetxt\n",
        "from numpy import genfromtxt\n",
        "\n",
        "from sklearn import cluster, datasets, mixture\n",
        "from sklearn.neighbors import kneighbors_graph\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from itertools import cycle, islice\n",
        "from tqdm import tqdm \n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch.utils import data\n",
        "import copy\n",
        "\n",
        "import argparse\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.autograd import Variable\n",
        "\n",
        "\n",
        "from IPython.core.debugger import set_trace\n",
        "\n",
        "from google.colab import drive,files,output\n",
        "# ===============\n",
        "# Initializations\n",
        "# ===============\n",
        "\n",
        "!pip install torch torchvision\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.4.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.5.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.12.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (7.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.18.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.4.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.5.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.12.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (7.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.18.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4Jqi0TGLXNLi"
      },
      "source": [
        "# Declaring Framework Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HFxR7S6TXM5o",
        "colab": {}
      },
      "source": [
        "# ===============\n",
        "# Parameters\n",
        "# ===============\n",
        "\n",
        "# --- General framework arguments\n",
        "\n",
        "args = {}\n",
        "kwargs = {}\n",
        "args['num_train_batch'] = 1  # number of MNIST training batches\n",
        "args['num_valid_batch'] = 10  # number of MNIST validation batches\n",
        "args['train_batch_size'] = 100   # training batch size\n",
        "args['valid_batch_size'] = 100   # validation batch size\n",
        "args['test_batch_size'] = 10\n",
        "# args['epochs'] = 100  # number of training epochs \n",
        "args['lr'] = 1 # 0.1  # 0.01 # learning rate # this is over-written by the solution\n",
        "args['momentum'] = 0.5 # SGD momentum (default: 0.5); momentum is a moving average of our gradients (helps keep a useful direction)\n",
        "args['clip_level'] = 0.5  # gradient clip threshold\n",
        "args['seed']= 1 #random seed\n",
        "args['log_interval_epoch'] = 1 # display training loss every log_int... epochs\n",
        "args['cuda'] = True \n",
        "args['patience'] = 1000  # stop train. if the valid. err. hasn't improved by this num. of epochs\n",
        "args['noise_in'] = -1   # 0.5  # 0.15  # amount of noise to add to the training data\n",
        "args['noise_out'] = -1   #  probability of changing an output label to some random label\n",
        "args['verbose_train'] = False   # print status of model training?\n",
        "args['verbose_meta'] = True #  print status of architecture optimization?\n",
        "args['min_inst_class'] = 5 # minimum number of instances per class in the training set\n",
        "args['batch_max_tries'] = 10 # max. num. of attempts in extracting data batches\n",
        "args['save_best_chrom'] = True # False  # save the best chromose in Google Drive?\n",
        "args['dc_prob_drop'] = 0.5 # 0.5 # probability of dropping a circuit (DropCircuit)\n",
        "args['prob_sel_branch'] = 0.5 # probability of architectural search selecting/using a branch (this is not DropCircuit) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WHRtBqQPXUgZ"
      },
      "source": [
        "# Declaring Neural Network Architecture Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OFe4d-SEXW0f",
        "colab": {}
      },
      "source": [
        "# --- Neural architectural limits\n",
        "\n",
        "args['num_epochs_search'] = 10 # 100  # number of epochs for training during architecture search \n",
        "args['num_epochs_test'] = 1000  # num. epochs for training during the final test\n",
        "limits = {}\n",
        "limits['min_layer_nodes'] =  10 # 5 # 50\n",
        "limits['max_layer_nodes'] = 100\n",
        "limits['max_pre_branch_layers'] = 5 # 3\n",
        "limits['max_branches'] = 10\n",
        "limits['max_branch_layers'] = 5  # 3\n",
        "limits['max_post_branch_layers'] = 5 # 3\n",
        "limits['lr1_min'] = 0.001\n",
        "limits['lr1_,max'] = 2\n",
        "limits['lr2_min'] = 0.001\n",
        "limits['lr2_,max'] = 2\n",
        "limits['moment_min'] = 0.001\n",
        "limits['moment_max'] = 1\n",
        "\n",
        "# np.random.seed(0)\n",
        "# torch.manual_seed(0)\n",
        "\n",
        "num_train_instances = args['num_train_batch'] * args['train_batch_size']\n",
        "num_valid_instances = args['num_valid_batch'] * args['valid_batch_size']\n",
        "\n",
        "data_rand_seed = 1 # (other pre-tested seeds: 2, 3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9pz5Mgv6X3mT"
      },
      "source": [
        "# Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "t3H0b-IqX4yu",
        "colab": {}
      },
      "source": [
        "# ============\n",
        "# Load dataset\n",
        "# ============\n",
        "\n",
        "# Function to be added here to load dataset\n",
        "def load_from_csv(path):\n",
        "    '''\n",
        "    Loads the csv file from path specified into a np array\n",
        "\n",
        "    Args:\n",
        "    path: Path where file is located\n",
        "    e.g path='gdrive/My Drive/Chromosome Saves/chrom_acc72'\n",
        "\n",
        "    Returns:\n",
        "    None\n",
        "    '''\n",
        "    drive.mount('/content/gdrive/')\n",
        "    some_file = np.genfromtxt(path, dtype='float', delimiter=',')\n",
        "\n",
        "    return some_file\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "O6Jaw59JYa8Z"
      },
      "source": [
        "# Seed Creation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "35Ewp2u7X97m",
        "colab": {}
      },
      "source": [
        "# Seed creation\n",
        "torch.manual_seed(data_rand_seed)\n",
        "np.random.seed(data_rand_seed)\n",
        "\n",
        "a_data_transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
        "  \n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=True, download=True,\n",
        "                   transform=a_data_transform),\n",
        "    batch_size=args['train_batch_size'], shuffle=True, **kwargs)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=False, transform=a_data_transform),\n",
        "    batch_size=args['test_batch_size'], shuffle=True, **kwargs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "P8OMy3wF1IIl"
      },
      "source": [
        "# Classes\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yxbyQvvq1MDK",
        "colab": {}
      },
      "source": [
        "# Simple layer for doing elementwise scaling\n",
        "# Adapated from https://stackoverflow.com/questions/51980654/pytorch-element-wise-filter-layer\n",
        "class Elem_Scaling_1D(nn.Module):\n",
        "  def __init__(self, num_nodes, bogus):  # clean-up \"bogus\"\n",
        "    super(Elem_Scaling_1D, self).__init__()\n",
        "    # Initialize\n",
        "    init_weights = np.random.normal(loc=0, scale=0.5, size=np.shape(num_nodes))\n",
        "    # Assign\n",
        "    self.weights = torch.from_numpy(init_weights)\n",
        "    #self.weights = nn.Parameter(torch.Tensor(1, num_nodes))  # trainable parameter\n",
        "\n",
        "  def forward(self, x):\n",
        "    # assuming x is of size num_inst-1-num_nodes\n",
        "    return x * self.weights  # element-wise multiplication\n",
        "\n",
        "# Class gradient-based neural diversity machine\n",
        "class GBNDM(nn.Module):   \n",
        "    def __init__(self, a_chromosome):   # assuming MNIST\n",
        "        super(GBNDM, self).__init__()\n",
        "\n",
        "        # --------- Pre-branch layers \n",
        "        self.chromosome = a_chromosome\n",
        "        self.pre_branch_layers = nn.ModuleList()\n",
        "        prev_num_out = 28*28\n",
        "        chrom_ind = 5 # skip over 3 learning rate param., 1 moment. p. (interp./used in training)\n",
        "        # and 1 exist-or-not (may use in future versions).\n",
        "        # Each layer is [exist-or-not, id-or-linear, activation function (AF), 2 AF parameters, number of nodes → total: 6 parameters]\n",
        "        for i in range(limits['max_pre_branch_layers']):\n",
        "          \n",
        "          # Extract and interpret parameters\n",
        "          layer_params_raw = a_chromosome[chrom_ind:chrom_ind+6]\n",
        "          layer_params_real = self.interp_layer_param(layer_params_raw)\n",
        "        \n",
        "          # Decide on whether to create a layer or not\n",
        "          if i==0:  # the first layer of each segment is done by default\n",
        "            do_layer = True\n",
        "          else:\n",
        "            do_layer = layer_params_real[0]\n",
        "\n",
        "          if do_layer:\n",
        "            # Create layer    \n",
        "            a_layer, prev_num_out = self.create_layer(prev_num_out, layer_params_real)\n",
        "            # Append layer and update chromosome index\n",
        "            self.pre_branch_layers.append(a_layer)\n",
        "            \n",
        "          chrom_ind += 6\n",
        "\n",
        "        # --------- Branches\n",
        "        num_out_pre_branch = prev_num_out\n",
        "\n",
        "        self.branches = nn.ModuleList()\n",
        "\n",
        "        # Scan over branches\n",
        "        final_num_nodes = []  # keep track of the size of the final layer of each branch\n",
        "        count_branches = 0\n",
        "        for bi in range(limits['max_branches']):\n",
        "\n",
        "          # Initialize branch\n",
        "          branch_layers = nn.ModuleList()\n",
        "\n",
        "          # Do branch? Always do the first one by default\n",
        "          if (a_chromosome[chrom_ind] < args['prob_sel_branch']) or (bi == 0):\n",
        "            do_branch = True\n",
        "          else: \n",
        "            do_branch = False\n",
        "            \n",
        "          chrom_ind += 1\n",
        "\n",
        "          # Scan over branch layers\n",
        "\n",
        "          if do_branch:  # if doing branch\n",
        "\n",
        "            this_fin_num_nodes = 0\n",
        "\n",
        "            count_branches += 1\n",
        "            \n",
        "            for li in range(limits['max_branch_layers']):\n",
        "\n",
        "              # Extract and interpret parameters\n",
        "              layer_params_raw = a_chromosome[chrom_ind:chrom_ind+6]\n",
        "              layer_params_real = self.interp_layer_param(layer_params_raw)\n",
        "\n",
        "              # Decide on whether to create a layer or not\n",
        "              if li==0:  # the first layer of each branch is done by default\n",
        "                do_layer = True\n",
        "                prev_num_out = num_out_pre_branch\n",
        "              else:\n",
        "                do_layer = layer_params_real[0]\n",
        "\n",
        "              if do_layer:\n",
        "                # print('... temp ... this_fin_num_nodes: {}.'.format(this_fin_num_nodes))\n",
        "                # Create layer    \n",
        "                a_layer, prev_num_out = self.create_layer(prev_num_out, layer_params_real)\n",
        "                # Num_nodes - keep storing the latest one; the last latest is the final layer num_nodes\n",
        "                this_fin_num_nodes = prev_num_out\n",
        "                # Append layer and update chromosome index\n",
        "                branch_layers.append(a_layer)\n",
        "\n",
        "              chrom_ind += 6\n",
        "\n",
        "            # Append branch\n",
        "            final_num_nodes.append(this_fin_num_nodes)\n",
        "            #print('final_num_nodes: {}.'.format(final_num_nodes))\n",
        "            self.branches.append(branch_layers)\n",
        "\n",
        "          else: # if not doing branch you still need to increment chromosome index\n",
        "\n",
        "            chrom_ind += 6*limits['max_branch_layers']\n",
        "\n",
        "        self.num_branches = count_branches\n",
        "        self.dc_prob_activ = 1 - args['dc_prob_drop'] # probability of using a circuit\n",
        "        self.tot_nodes_branch_final = sum(final_num_nodes)\n",
        "        \n",
        "        # --------- Post-branch layers\n",
        "\n",
        "        prev_num_out = self.tot_nodes_branch_final\n",
        "        \n",
        "        # --- Create post-branch layers\n",
        "\n",
        "        chrom_ind += 1  # skip over the parameter pertaining to the existence or not of the post-branch segment\n",
        "\n",
        "        self.post_branch_layers = nn.ModuleList()\n",
        "        \n",
        "        # Each layer is [exist-or-not, id-or-linear, activation function (AF), 2 AF parameters, number of nodes → total: 6 parameters]\n",
        "        for li in range(limits['max_post_branch_layers']):\n",
        "          \n",
        "          # Extract and interpret parameters\n",
        "          layer_params_raw = a_chromosome[chrom_ind:chrom_ind+6]\n",
        "          layer_params_real = self.interp_layer_param(layer_params_raw)\n",
        "        \n",
        "          # Decide on whether to create a layer or not\n",
        "          if li==0:  # the first layer of each segment is done by default\n",
        "            do_layer = True\n",
        "          else:\n",
        "            do_layer = layer_params_real[0]\n",
        "\n",
        "          if do_layer:\n",
        "            # Create layer    \n",
        "            a_layer, prev_num_out = self.create_layer(prev_num_out, layer_params_real)\n",
        "            # Append layer and update chromosome index\n",
        "            self.post_branch_layers.append(a_layer)\n",
        "            \n",
        "          chrom_ind += 6\n",
        "\n",
        "        # Create a final layer\n",
        "        self.final_layer = nn.Linear(prev_num_out, 10)\n",
        "\n",
        "    def forward(self, x, dc_mask = None):\n",
        "        \n",
        "        x = x.view(-1, 28*28)\n",
        "\n",
        "        # Apply pre-branch layers\n",
        "        for pi, a_layer in enumerate(self.pre_branch_layers):\n",
        "          #print('Pre-layer {}'.format(pi))\n",
        "          # if isinstance(a_layer, Elem_Scaling_1D):\n",
        "          #   set_trace()\n",
        "          x = a_layer(x)\n",
        "\n",
        "        # Apply branches\n",
        "        branch_finals = []\n",
        "        for bi, a_branch in enumerate(self.branches):\n",
        "          #print('Branch {}'.format(bi))\n",
        "          \n",
        "          z = x\n",
        "\n",
        "          for a_layer in a_branch:\n",
        "            # if isinstance(a_layer, Elem_Scaling_1D):\n",
        "            #   set_trace()\n",
        "            z = a_layer(z)\n",
        "          \n",
        "          if self.training:\n",
        "            z = (dc_mask[bi] * z) / self.dc_prob_activ  # if training apply DropCircuit\n",
        "          \n",
        "          branch_finals.append(z)\n",
        "\n",
        "        # Concatenate multi-branch final layers\n",
        "        x = torch.cat(branch_finals,dim=1)\n",
        "\n",
        "        # if not(self.training): # if not training then testing/evaluating\n",
        "        #   x = self.dc_prob_activ * x  # scaling due to DropCircuit\n",
        "      \n",
        "\n",
        "        # Apply post-branch layers\n",
        "        for pi, a_layer in enumerate(self.post_branch_layers):\n",
        "          #print('Post-layer: {}'.format(pi))\n",
        "          # if isinstance(a_layer, Elem_Scaling_1D):\n",
        "          #   set_trace()\n",
        "          x = a_layer(x)\n",
        "\n",
        "        # Apply final layer\n",
        "        x = self.final_layer(x)\n",
        "               \n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "\n",
        "    # Method for interpreting layer parameters\n",
        "    # Each layer is [exist-or-not, id-or-linear, activation function (AF), 2 AF parameters, number of nodes → total: 6 parameters]\n",
        "    def interp_layer_param(self, layer_params_raw):\n",
        "      # Exist or not\n",
        "      if layer_params_raw[0] < 0.5:\n",
        "        exist = False\n",
        "      else:\n",
        "        exist = True\n",
        "      # Weight function\n",
        "      tot_weight_func = 3\n",
        "      if layer_params_raw[1] < 0.6: # or ... (1/tot_weight_func):  \n",
        "        weight_func_sel = 'linear'\n",
        "      elif layer_params_raw[1] < 0.8: # or ... (2/tot_weight_func):\n",
        "        weight_func_sel = 'elem_scale'\n",
        "      else:\n",
        "        weight_func_sel = 'id'\n",
        "      # activation function\n",
        "      tot_node_func = 22\n",
        "      if layer_params_raw[2] < (1/tot_node_func):\n",
        "        activ_func = 'ReLU'\n",
        "        param_1 = -1\n",
        "        param_2 = -1\n",
        "      elif layer_params_raw[2] < (2/tot_node_func):\n",
        "        activ_func = 'Hardshrink'\n",
        "        param_1 = scale_to_range(layer_params_raw[3],0,2)\n",
        "        param_2 = -1\n",
        "      elif layer_params_raw[2] < (3/tot_node_func):\n",
        "        activ_func = 'Hardtanh'\n",
        "        param_1 = scale_to_range(layer_params_raw[3],0,2)\n",
        "        param_2 = scale_to_range(layer_params_raw[4],0,2)\n",
        "        if param_1 > param_2:  # param_1 is min_val; param_2 is max_val\n",
        "          tmp_val = param_1\n",
        "          param_1 = param_2\n",
        "          param_2 = tmp_val\n",
        "        elif param_1 == param_2:\n",
        "          param_2 += 0.1\n",
        "      elif layer_params_raw[2] < (4/tot_node_func):\n",
        "        activ_func = 'LeakyReLU'\n",
        "        param_1 = scale_to_range(layer_params_raw[3],0,1)\n",
        "        param_2 = -1\n",
        "      elif layer_params_raw[2] < (5/tot_node_func):\n",
        "        activ_func = 'LogSigmoid'\n",
        "        param_1 = -1\n",
        "        param_2 = -1\n",
        "      elif layer_params_raw[2] < (6/tot_node_func):\n",
        "        activ_func = 'PReLU'\n",
        "        param_1 = -1\n",
        "        param_2 = -1\n",
        "      elif layer_params_raw[2] < (7/tot_node_func):\n",
        "        activ_func = 'ELU'\n",
        "        param_1 = scale_to_range(layer_params_raw[3],0,2)\n",
        "        param_2 = -1\n",
        "      elif layer_params_raw[2] < (8/tot_node_func):\n",
        "        activ_func = 'ReLU6'\n",
        "        param_1 = -1\n",
        "        param_2 = -1\n",
        "      elif layer_params_raw[2] < (9/tot_node_func):\n",
        "        activ_func = 'RReLU'\n",
        "        param_1 = -1\n",
        "        param_2 = -1\n",
        "      elif layer_params_raw[2] < (10/tot_node_func):\n",
        "        activ_func = 'SELU'\n",
        "        param_1 = -1\n",
        "        param_2 = -1\n",
        "      elif layer_params_raw[2] < (11/tot_node_func):\n",
        "        activ_func = 'CELU'\n",
        "        param_1 = scale_to_range(layer_params_raw[3],0,2)\n",
        "        param_2 = -1\n",
        "      elif layer_params_raw[2] < (12/tot_node_func):\n",
        "        activ_func = 'GELU'\n",
        "        param_1 = -1\n",
        "        param_2 = -1\n",
        "      elif layer_params_raw[2] < (13/tot_node_func):\n",
        "        activ_func = 'Sigmoid'\n",
        "        param_1 = -1\n",
        "        param_2 = -1\n",
        "      elif layer_params_raw[2] < (14/tot_node_func):\n",
        "        activ_func = 'Softplus'\n",
        "        param_1 = scale_to_range(layer_params_raw[3],0,2)\n",
        "        param_2 = scale_to_range(layer_params_raw[4],0,40)\n",
        "      elif layer_params_raw[2] < (15/tot_node_func):\n",
        "        activ_func = 'Softshrink'\n",
        "        param_1 = scale_to_range(layer_params_raw[3],0,2)\n",
        "        param_2 = -1\n",
        "      elif layer_params_raw[2] < (16/tot_node_func):\n",
        "        activ_func = 'Softsign'\n",
        "        param_1 = -1\n",
        "        param_2 = -1\n",
        "      elif layer_params_raw[2] < (17/tot_node_func):\n",
        "        activ_func = 'Tanh'\n",
        "        param_1 = -1\n",
        "        param_2 = -1\n",
        "      elif layer_params_raw[2] < (18/tot_node_func):\n",
        "        activ_func = 'Tanhshrink'\n",
        "        param_1 = -1\n",
        "        param_2 = -1\n",
        "      elif layer_params_raw[2] < (19/tot_node_func):\n",
        "        activ_func = 'Threshold'\n",
        "        param_1 = scale_to_range(layer_params_raw[3],0,2)\n",
        "        param_2 = scale_to_range(layer_params_raw[4],0,2)\n",
        "      elif layer_params_raw[2] < (20/tot_node_func):\n",
        "        activ_func = 'Softmin'\n",
        "        param_1 = -1\n",
        "        param_2 = -1\n",
        "      elif layer_params_raw[2] < (21/tot_node_func):\n",
        "        activ_func = 'Softmax'\n",
        "        param_1 = -1\n",
        "        param_2 = -1\n",
        "      else: \n",
        "        activ_func = 'None'\n",
        "        param_1 = -1\n",
        "        param_2 = -1\n",
        "\n",
        "      # number of nodes\n",
        "      num_nodes = scale_to_range(layer_params_raw[5], limits['min_layer_nodes'], limits['max_layer_nodes'])\n",
        "      num_nodes = num_nodes.astype(int)\n",
        "\n",
        "      return (exist, weight_func_sel, activ_func, param_1, param_2, num_nodes)\n",
        "\n",
        "    # Method for creating a layer\n",
        "    # layer_params_real format: (exist, weight_func_sel, activ_func, param_1, param_2, num_nodes)\n",
        "    def create_layer(self, prev_num_out, layer_params_real):\n",
        "      \n",
        "      exist, weight_func_sel, activ_func, param_1, param_2, num_nodes = layer_params_real\n",
        "      \n",
        "      # wf_param1/wf_param2 --> not elegant \n",
        "      if weight_func_sel == 'linear':\n",
        "        weight_func = nn.Linear\n",
        "        num_nodes_in = prev_num_out\n",
        "        num_nodes_out = num_nodes\n",
        "        wf_param1 = num_nodes_in\n",
        "        wf_param2 = num_nodes_out\n",
        "      elif weight_func_sel == 'id':\n",
        "        weight_func = nn.Identity\n",
        "        num_nodes_in = prev_num_out\n",
        "        num_nodes_out = prev_num_out\n",
        "        wf_param1 = num_nodes_in\n",
        "        wf_param2 = num_nodes_out\n",
        "      else:  # 'elem_scale'\n",
        "        weight_func = Elem_Scaling_1D\n",
        "        num_nodes_in = prev_num_out\n",
        "        num_nodes_out = prev_num_out\n",
        "        wf_param1 = num_nodes_in\n",
        "        wf_param2 = num_nodes_out\n",
        "        \n",
        "      if activ_func == 'ReLU':\n",
        "        a_layer = nn.Sequential(\n",
        "            weight_func(wf_param1, wf_param2),\n",
        "            nn.ReLU())\n",
        "      elif activ_func == 'Hardshrink':\n",
        "        a_layer = nn.Sequential(\n",
        "            weight_func(wf_param1, wf_param2),\n",
        "            nn.Hardshrink(param_1))\n",
        "      elif activ_func == 'Hardtanh':\n",
        "        a_layer = nn.Sequential(\n",
        "            weight_func(wf_param1, wf_param2),\n",
        "            nn.Hardtanh(param_1, param_2))\n",
        "      elif activ_func == 'LeakyReLU':\n",
        "        a_layer = nn.Sequential(\n",
        "            weight_func(wf_param1, wf_param2),\n",
        "            nn.LeakyReLU(param_1))\n",
        "      elif activ_func == 'LogSigmoid':\n",
        "        a_layer = nn.Sequential(\n",
        "            weight_func(wf_param1, wf_param2),\n",
        "            nn.LogSigmoid())\n",
        "      elif activ_func == 'PReLU':\n",
        "        a_layer = nn.Sequential(\n",
        "            weight_func(wf_param1, wf_param2),\n",
        "            nn.PReLU())\n",
        "      elif activ_func == 'ELU':\n",
        "        a_layer = nn.Sequential(\n",
        "            weight_func(wf_param1, wf_param2),\n",
        "            nn.ELU(param_1))\n",
        "      elif activ_func == 'ReLU6':\n",
        "        a_layer = nn.Sequential(\n",
        "            weight_func(wf_param1, wf_param2),\n",
        "            nn.ReLU6())\n",
        "      elif activ_func == 'RReLU':\n",
        "        a_layer = nn.Sequential(\n",
        "            weight_func(wf_param1, wf_param2),\n",
        "            nn.RReLU())\n",
        "      elif activ_func == 'SELU':\n",
        "        a_layer = nn.Sequential(\n",
        "            weight_func(wf_param1, wf_param2),\n",
        "            nn.SELU())\n",
        "      elif activ_func == 'CELU':\n",
        "        a_layer = nn.Sequential(\n",
        "            weight_func(wf_param1, wf_param2),\n",
        "            nn.CELU(param_1))\n",
        "      elif activ_func == 'GELU':\n",
        "        a_layer = nn.Sequential(\n",
        "            weight_func(wf_param1, wf_param2),\n",
        "            nn.ReLU())  # for some reason GELU is not present; revert later if relevant\n",
        "      elif activ_func == 'Sigmoid':\n",
        "        a_layer = nn.Sequential(\n",
        "            weight_func(wf_param1, wf_param2),\n",
        "            nn.Sigmoid())\n",
        "      elif activ_func == 'Softplus':\n",
        "        a_layer = nn.Sequential(\n",
        "            weight_func(wf_param1, wf_param2),\n",
        "            nn.Softplus(param_1, param_2))\n",
        "      elif activ_func == 'Softshrink':\n",
        "        a_layer = nn.Sequential(\n",
        "            weight_func(wf_param1, wf_param2),\n",
        "            nn.Softplus(param_1))\n",
        "      elif activ_func == 'Softsign':\n",
        "        a_layer = nn.Sequential(\n",
        "            weight_func(wf_param1, wf_param2),\n",
        "            nn.Softsign())\n",
        "      elif activ_func == 'Tanh':\n",
        "        a_layer = nn.Sequential(\n",
        "            weight_func(wf_param1, wf_param2),\n",
        "            nn.Tanh())\n",
        "      elif activ_func == 'Tanhshrink':\n",
        "        a_layer = nn.Sequential(\n",
        "            weight_func(wf_param1, wf_param2),\n",
        "            nn.Tanhshrink())\n",
        "      elif activ_func == 'Threshold':\n",
        "        a_layer = nn.Sequential(\n",
        "            weight_func(wf_param1, wf_param2),\n",
        "            nn.Threshold(param_1, param_2))\n",
        "      elif activ_func == 'Softmin':\n",
        "        a_layer = nn.Sequential(\n",
        "            weight_func(wf_param1, wf_param2),\n",
        "            nn.Softmin())\n",
        "      elif activ_func == 'Softmax':\n",
        "        a_layer = nn.Sequential(\n",
        "            weight_func(wf_param1, wf_param2),\n",
        "            nn.Softmax())\n",
        "      else:\n",
        "        a_layer = weight_func(wf_param1, wf_param2)\n",
        "\n",
        "      return a_layer, num_nodes_out\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "itPKtM0B1tNY",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DaRvVV5mDddf"
      },
      "source": [
        "# Training and Test Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "R7H5AURDDe1w",
        "colab": {}
      },
      "source": [
        "# ==========================\n",
        "# Train and test functions\n",
        "# ==========================\n",
        "\n",
        "# Compute the number of parameters in a chromose (depends on limits)\n",
        "def comp_num_chrom_param(limits):\n",
        "  # Num. of training parameters\n",
        "  tot_train_param = 4 # lr1, lr2, lr2 prop, momentum\n",
        "  # Num. of pre-branch parameters\n",
        "  tot_pre_branch = 1+(6*limits['max_pre_branch_layers']) # exist-or-not, layer params.\n",
        "  # Num. of branch parameters\n",
        "  tot_branch = (1+(6*limits['max_branch_layers']))*limits['max_branches']\n",
        "  # Num. of post-branch parameters\n",
        "  tot_post_branch = 1+(6*limits['max_post_branch_layers'])\n",
        "\n",
        "  return tot_train_param+tot_pre_branch+tot_branch+tot_post_branch\n",
        "\n",
        "# Interpret learning rate and momentum parameters\n",
        "def interp_lrm(params):\n",
        "  lr1 = scale_to_range(params[0], limits['lr1_min'], limits['lr1_,max'])\n",
        "  lr2 = scale_to_range(params[1], limits['lr2_min'], limits['lr2_,max'])\n",
        "  if lr1 < lr2:\n",
        "    temp = lr1\n",
        "    lr1 = lr2\n",
        "    lr2 = temp\n",
        "  lr2_epoch = (np.round(params[2]*args['num_epochs_search'])).astype(int)\n",
        "  a_decr = (lr1-lr2)/lr2_epoch\n",
        "  a_momentum = scale_to_range(params[3], limits['moment_min'], limits['moment_max'])\n",
        "  return lr1, lr2, lr2_epoch, a_momentum, a_decr\n",
        "\n",
        "# Function to train a specific model\n",
        "# Early stopping, or returning best validation model, is not implemented \n",
        "def do_training(a_model, train_params, num_epochs):\n",
        "\n",
        "  lr1, lr2, lr2_epoch, a_momentum, lr_decr = train_params  \n",
        "  args['lr'] = lr1\n",
        "  args['momentum'] = a_momentum\n",
        "\n",
        "  optimizer = optim.SGD(a_model.parameters(), lr=args['lr'], momentum=args['momentum'])\n",
        "  best_valid_accur = 0\n",
        "  best_model = type(a_model)(a_model.chromosome) # get a new instance\n",
        "  valid_accurs = []\n",
        "  train_accurs = []\n",
        "  train_start_time = time.time()\n",
        "  for epoch in range(1, num_epochs + 1):\n",
        "\n",
        "    if args['verbose_train']:\n",
        "      print('Epoch {} learning rate: {}.'.format(epoch, args['lr']))\n",
        "\n",
        "    train_one_epoch(a_model, optimizer, epoch, train_batches)\n",
        "    \n",
        "    a_train_accur = comp_accuracy(a_model, train_batches, num_train_instances)\n",
        "    if args['verbose_train']:\n",
        "      print('Training accuracy: {}%.'.format(a_train_accur))\n",
        "    train_accurs.append(a_train_accur)\n",
        "    \n",
        "    a_valid_accur = comp_accuracy(a_model, valid_batches, num_valid_instances)\n",
        "    if args['verbose_train']:\n",
        "      print('Validation accuracy: {}%.'.format(a_valid_accur))\n",
        "    valid_accurs.append(a_valid_accur)\n",
        "    if a_valid_accur > best_valid_accur:\n",
        "      best_valid_accur = a_valid_accur\n",
        "      # best_model.load_state_dict(a_model.state_dict()) # copy weights and stuff\n",
        "      best_model = copy.deepcopy(a_model)\n",
        "\n",
        "    # Decrement learning rate\n",
        "    if epoch < lr2_epoch:\n",
        "        args['lr'] -= lr_decr\n",
        "        for param_group in optimizer.param_groups:\n",
        "          param_group['lr'] = args['lr']\n",
        "\n",
        "  train_elapsed_time = time.time() - train_start_time\n",
        "#   print('The training process took {} seconds.'.format(train_elapsed_time))\n",
        "\n",
        "  if args['cuda']:\n",
        "      best_model.cuda()\n",
        "\n",
        "  return a_model, best_model, valid_accurs, train_accurs\n",
        "  \n",
        "# Create a random mask for DropCircuit\n",
        "def make_mask(num_circuits, prob_drop):\n",
        "  rand_vals = np.random.rand(num_circuits)\n",
        "  decisions = rand_vals >= prob_drop\n",
        "  a_mask = np.ones(num_circuits)*decisions\n",
        "  # Must have at least one circuit active\n",
        "  if np.sum(a_mask) == 0:\n",
        "    rand_index = np.random.randint(num_circuits)\n",
        "    a_mask[rand_index] = 1.0\n",
        "\n",
        "  return torch.from_numpy(a_mask)\n",
        "\n",
        "def train_one_epoch(a_model, optimizer, epoch, batches):\n",
        "\n",
        "  a_model.train()\n",
        "  for batch_idx, (data, target) in enumerate(batches):\n",
        "      if args['cuda']:\n",
        "          data, target = data.cuda(), target.cuda()\n",
        "      # Variables in Pytorch are differentiable. \n",
        "      \n",
        "      data, target = Variable(data), Variable(target)\n",
        "      #This will zero out the gradients for this batch. \n",
        "      optimizer.zero_grad()\n",
        "      \n",
        "      # Create DropCircuit mask\n",
        "      dc_mask = make_mask(a_model.num_branches, args['dc_prob_drop'])\n",
        "      \n",
        "      output = a_model(data, dc_mask)\n",
        "\n",
        "      # Calculate the negative log likelihood loss.\n",
        "      loss = F.nll_loss(output, target)\n",
        "      #dloss/dx for every Variable \n",
        "      loss.backward()\n",
        "      # Gradient clipping\n",
        "      torch.nn.utils.clip_grad_norm_(a_model.parameters(), args['clip_level'])\n",
        "      #to do a one-step update on our parameter.\n",
        "      optimizer.step()\n",
        "      #Print out the loss periodically. \n",
        "\n",
        "  if args['verbose_train']:\n",
        "    if epoch % args['log_interval_epoch'] == 0:\n",
        "      print('Epoch: {}. Latest loss: {}.'.format(epoch, loss.data))\n",
        "\n",
        "# Compute accuracy\n",
        "# The argument (data_source) of this function can be a data_loader or a list of batches (previously extracted) \n",
        "def comp_accuracy(a_model, data_source, src_num_instances):\n",
        "    a_model.eval()\n",
        "    a_loss = 0\n",
        "    correct = 0\n",
        "    preds = torch.zeros(0)\n",
        "    first = True\n",
        "    for a_data_in, a_data_out in data_source:\n",
        "        if args['cuda']:\n",
        "            a_data_in, a_data_out = a_data_in.cuda(), a_data_out.cuda()\n",
        "        a_data_in, a_data_out = Variable(a_data_in), Variable(a_data_out)\n",
        "        output = a_model(a_data_in)\n",
        "\n",
        "        a_loss += F.nll_loss(output, a_data_out, reduction='sum').data # sum up batch loss\n",
        "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
        "        if first:\n",
        "          preds = pred\n",
        "          first = False\n",
        "        else:\n",
        "          preds = torch.cat((preds, pred),0)\n",
        "\n",
        "        correct += pred.eq(a_data_out.data.view_as(pred)).long().cpu().sum()\n",
        "\n",
        "    # Compute and return accuracy\n",
        "    if type(data_source) == list:  # case: list of batches\n",
        "      result = 100. * (correct.numpy()/ src_num_instances)\n",
        "    else: # case: data_loader\n",
        "      result = 100. * (correct.numpy()/ len(data_source.dataset))\n",
        "    return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HRaTOxAyyXq2"
      },
      "source": [
        "# Data Prep, Visualization, Gen Chrom\n",
        "> Indented block\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QPssuRVkyaWZ",
        "colab": {}
      },
      "source": [
        "# Check whether we have enough instances per class\n",
        "# We want an imbalanced dataset but we don't want any specific label having\n",
        "# an \"insufficient\" number of instances.\n",
        "def check_enough_inst(batches, min_inst_per_class):\n",
        "  \n",
        "  # Concatenate training batch output labels\n",
        "  labels = batches[0][1].numpy()\n",
        "  for batch_i in range(1,args['num_train_batch']):\n",
        "    new_labels = batches[batch_i][1].numpy()\n",
        "    labels = np.concatenate((labels, new_labels))\n",
        "\n",
        "  labels = labels.tolist()\n",
        "\n",
        "  # Scan though labels\n",
        "  for a_label in range(10):  # assuming MNIST, of course\n",
        "    a_count = labels.count(a_label)\n",
        "    # If label count is < min_inst_per_class return False\n",
        "    if a_count < min_inst_per_class:\n",
        "      return False\n",
        "\n",
        "  # Return True\n",
        "  return True\n",
        "\n",
        "\n",
        "# Prepare data. Extract training and validation batches\n",
        "# This is where we make the problem \"small and imbalanced\"\n",
        "def extract_batches(a_loader, min_inst_per_class, max_tries):\n",
        "\n",
        "  # Keep trying until you have a required minimum number of instances \n",
        "  # per class in the training set (not elegant but ok for the range of \n",
        "  # \"min_inst_per_class\" we need)\n",
        "\n",
        "  for ti in range(max_tries):\n",
        "\n",
        "    print('Data extraction trial {}.'.format(ti))\n",
        "\n",
        "    # Initializations\n",
        "    train_batches = []\n",
        "    valid_batches = []\n",
        "    tot_batch_extract = args['num_train_batch'] + args['num_valid_batch']\n",
        "\n",
        "    # Extract\n",
        "    for batch_idx, (data, target) in enumerate(a_loader):\n",
        "\n",
        "      if batch_idx < args['num_train_batch']:\n",
        "        train_batches.append((data,target))\n",
        "      else:\n",
        "        valid_batches.append((data,target))\n",
        "        if batch_idx == tot_batch_extract - 1:\n",
        "          break\n",
        "\n",
        "    # Check minimum number of instances\n",
        "    enough_instaces = check_enough_inst(train_batches, min_inst_per_class)\n",
        "    if enough_instaces:\n",
        "      return train_batches, valid_batches\n",
        "\n",
        "  print('It was not possible to create a dataset.')\n",
        "  print('Consider increasing the overall number of instances, or')\n",
        "  print('decreasing the minimum instances per class allowed.')\n",
        "  return [], []\n",
        "\n",
        "# ===========================\n",
        "# Display histogram of labels\n",
        "# ===========================\n",
        "\n",
        "def disp_hist_labaels(batches):\n",
        "  # Concatenate training batch output labels\n",
        "  labels = batches[0][1].numpy()\n",
        "  for batch_i in range(1,args['num_train_batch']):\n",
        "    new_labels = batches[batch_i][1].numpy()\n",
        "    labels = np.concatenate((labels, new_labels))\n",
        "\n",
        "  num_bins = 10\n",
        "  n, bins, patches = plt.hist(labels, num_bins, facecolor='blue', alpha=0.5)\n",
        "  plt.show()\n",
        "\n",
        "# This is not currently used\n",
        "\n",
        "# Display MNIST instances\n",
        "# Adapted from # https://github.com/CSCfi/machine-learning-scripts/blob/master/notebooks/pytorch-mnist-mlp.ipynb\n",
        "def disp_MNIST_inst(num_disp, X_train, y_train):\n",
        "  pltsize=1\n",
        "  plt.figure(figsize=(num_disp*pltsize, pltsize))\n",
        "  for i in range(num_disp):\n",
        "    plt.subplot(1,num_disp,i+1)\n",
        "    plt.axis('off')\n",
        "    plt.imshow(X_train[i,:,:].numpy().reshape(28,28), cmap=\"gray_r\")\n",
        "    plt.title('Class: '+str(y_train[i].item()))\n",
        "\n",
        "# Simple function to scale parameters\n",
        "# num is assume to be \\in [0,1]\n",
        "def scale_to_range(num,min_val,max_val):\n",
        "  range = max_val - min_val\n",
        "  return (num*range)+min_val\n",
        "\n",
        "# Generate a random chromosome where each param. is \\in [0,1)\n",
        "def gen_rand_chromosome(num_param):\n",
        "  chrom = np.random.rand(num_param)\n",
        "  return chrom"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rFnBFgWOj_tc",
        "outputId": "8cb104f6-8c7d-4030-a8fd-8128892eab38",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "test = np.random.rand(4)\n",
        "\n",
        "print(test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[4.17022005e-01 7.20324493e-01 1.14374817e-04 3.02332573e-01]\n",
            "[4.17022005e-01 7.20324493e-01 1.14374817e-04 3.02332573e-01]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "COLsvqUbw_sY"
      },
      "source": [
        "# Display Data Distribution\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aAZmGY0M18aG",
        "outputId": "c9f4d086-b842-423e-c158-a9d93792ef03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 604
        }
      },
      "source": [
        "train_batches, valid_batches = extract_batches(train_loader, args['min_inst_class'], args['batch_max_tries'])\n",
        "if args['verbose_train']:\n",
        "  print('Extracted {} train_batches, and {} valid_batches.'.format(len(train_batches), len(valid_batches)))\n",
        "\n",
        "\n",
        "disp_hist_labaels(train_batches)\n",
        "\n",
        "# ==========================\n",
        "# Display dataset\n",
        "# ==========================\n",
        "\n",
        "\n",
        "if args['verbose_train']:\n",
        "  X_train = train_batches[0][0]\n",
        "  y_train = train_batches[0][1]\n",
        "\n",
        "  disp_MNIST_inst(10, X_train, y_train)\n",
        "\n",
        "  sum_train_0 = X_train[0,:,:].sum()\n",
        "  min_train_0 = X_train[0,:,:].min()\n",
        "  max_train_0 = X_train[0,:,:].max()\n",
        "\n",
        "  print('X_train[0,:,:] --> sum ({}); min ({}); max ({}).'.format(sum_train_0, min_train_0, max_train_0))\n",
        "\n",
        "# ==========================\n",
        "# Design model\n",
        "# ==========================\n",
        "\n",
        "# \"Unseed\" the rest\n",
        "np.random.seed()\n",
        "a_rand_seed = np.random.randint(0,999999)\n",
        "torch.manual_seed(a_rand_seed)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data extraction trial 0.\n",
            "Data extraction trial 1.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUuMMeklqnEEvSY37f09gaa2DGv3LAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7efcc8cf3af0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        },
        {
          "output_type": "stream",
          "text": [
            "Data extraction trial 0.\n",
            "Data extraction trial 1.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUuMMeklqnEEvSY37f09gaa2DGv3LAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7efcc8cf3af0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mOXSLvfykNFo"
      },
      "source": [
        "# **Work To be Done Starts Here**\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9T7bv9bDEiXo"
      },
      "source": [
        "## Prepare Model, Neighbour Creation Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QC-I7PqHEkeo",
        "colab": {}
      },
      "source": [
        "# Prepare model\n",
        "def prepare_model(a_rand_chrom):\n",
        "    # Initialize chromosome and model\n",
        "    model = GBNDM(a_rand_chrom)\n",
        "  \n",
        "    if args['cuda']:\n",
        "        model.cuda()\n",
        "\n",
        "    # Interpret learning rates and momentum\n",
        "    lr1, lr2, lr2_epoch, a_momentum, lr_decr = interp_lrm(a_rand_chrom[0:4])\n",
        "    train_params = (lr1, lr2, lr2_epoch, a_momentum, lr_decr)\n",
        "    args['momentum'] = a_momentum\n",
        "    if args['verbose_train']:\n",
        "        print('lr1: {}'.format(lr1))\n",
        "        print('lr2: {}'.format(lr2))\n",
        "        print('lr2_epoch: {}'.format(lr2_epoch))\n",
        "        print('a_momentum: {}'.format(a_momentum))\n",
        "        print('lr_decr: {}'.format(lr_decr))\n",
        "\n",
        "    return model, train_params\n",
        "\n",
        "# Function for creating one neighbour\n",
        "def create_a_neighbour(a_chromosome, neighbour_range, num_chrom_params):\n",
        "    \n",
        "    mutat_vec = (np.random.rand(num_chrom_params)*neighbour_range)-(neighbour_range/2)\n",
        "\n",
        "    # Add mutation vector\n",
        "    new_chromosome = a_chromosome + mutat_vec\n",
        "\n",
        "    # Clip\n",
        "    np.clip(new_chromosome, 0, 0.99999999999, out=new_chromosome)\n",
        "\n",
        "    return new_chromosome\n",
        "\n",
        "      \n",
        "# Function for creating a list of neighbours\n",
        "def create_neighbours(a_chromosome, meta, num_chrom_params):\n",
        "    neighbours = []\n",
        "    # Scan through number of neighbours\n",
        "    for ni in range(meta['num_neighbours']):\n",
        "        # Create neighbour\n",
        "        a_neighb = create_a_neighbour(a_chromosome, meta['neighbour_range'], num_chrom_params)\n",
        "        \n",
        "        # Append neighbour\n",
        "        neighbours.append(a_neighb)\n",
        "\n",
        "    return neighbours\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Z7JIg_V2E3t5"
      },
      "source": [
        "## Differential Search Function (To be looked at and worked on)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "afmMhfa-E5bG",
        "colab": {}
      },
      "source": [
        "# Simple differential search\n",
        "def do_diff_chrom_v1(mat_chrom_accur, num_new_sol):\n",
        "  \n",
        "  # Initialize new chromosomes\n",
        "    num_chrom = mat_chrom_accur.shape[0]\n",
        "    if num_new_sol >= num_chrom:\n",
        "        num_new_sol = num_chrom-1\n",
        "    new_chromosomes = []\n",
        "\n",
        "    # Sort the array of chromosomes based on the first column (contains accur.)\n",
        "    mat_chrom_accur = mat_chrom_accur[(-mat_chrom_accur[:,0]).argsort()]\n",
        "    # print(mat_chrom_accur[:,0:4])\n",
        "\n",
        "    # Extract first/best chromosome\n",
        "    best_chrom = mat_chrom_accur[0,1:]\n",
        "\n",
        "    # Scan through new solutions\n",
        "    for si in range(num_new_sol):  \n",
        "\n",
        "        # Extract next best chrom\n",
        "        next_best_chrom = mat_chrom_accur[1+si,1:]\n",
        "\n",
        "        # Compute differential\n",
        "        a_diff = best_chrom - next_best_chrom\n",
        "\n",
        "        # Add differential whilst applying a learning rate\n",
        "        a_new_chrom = best_chrom + (meta['diff_lr']*a_diff)\n",
        "        np.clip(a_new_chrom, 0, 0.99999999999, out=a_new_chrom)\n",
        "\n",
        "        # Store new solution \n",
        "        new_chromosomes.append(a_new_chrom)\n",
        "\n",
        "    return new_chromosomes\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PlUQsx5tFYB_"
      },
      "source": [
        "## Model Test Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cs3abLvjFaRe",
        "colab": {}
      },
      "source": [
        "# ==========================\n",
        "# Test final model / Visualize predictions\n",
        "# ==========================\n",
        "\n",
        "def final_test(a_model):\n",
        "    a_model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    test_preds = torch.zeros(0)\n",
        "    first = True\n",
        "    for test_data_in, test_data_out in test_loader: \n",
        "\n",
        "        if args['cuda']:\n",
        "            test_data_in, test_data_out = test_data_in.cuda(), test_data_out.cuda()\n",
        "                \n",
        "        test_data_in, test_data_out = Variable(test_data_in), Variable(test_data_out)\n",
        "        output = a_model(test_data_in)\n",
        "        test_loss += F.nll_loss(output, test_data_out, reduction='sum').data # sum up batch loss\n",
        "\n",
        "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
        "        if first:\n",
        "          test_preds = pred\n",
        "          first = False\n",
        "        else:\n",
        "          test_preds = torch.cat((test_preds, pred),0)\n",
        "\n",
        "        correct += pred.eq(test_data_out.data.view_as(pred)).long().cpu().sum()\n",
        "\n",
        "    # Print test accuracy\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    accuracy = 100. * correct / len(test_loader.dataset)\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy (at final epoch): {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset), accuracy))\n",
        "\n",
        "    return accuracy\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TXbK2v51xMpH"
      },
      "source": [
        "## Graph Plotting, Metric Calculation Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tZSjpMv1NyiC",
        "colab": {}
      },
      "source": [
        "def plot_bar(x_axis, y_axis):\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_axes([0,0,1,1])\n",
        "    x_axis = ['Iteration {}'.format(i) for i in x_axis]\n",
        "    ax.bar(x_axis, height = y_axis, width = 0.5)\n",
        "    ax.set_ylabel('Accuracy')\n",
        "    # ax.set_ylim(bottom = (0,100))\n",
        "    ax.set_ybound(lower = 0, upper = 100)\n",
        "    # ax.set_yticks(y_axis) # displays the exact ticks\n",
        "    plt.show()\n",
        "\n",
        "def plot_bar_2(x_axis, y_axis):\n",
        "\n",
        "    # Specifies the positions on the x_axis for ticks to be labeled\n",
        "    y_pos = np.arange(len(x_axis))\n",
        "\n",
        "    # Formatting for the x_axis labels\n",
        "    objects = ['Trial {}'.format(i+1) for i in x_axis]\n",
        "\n",
        "    # Calculate metrics for error bar handling\n",
        "    std_dev, mean, variance = calculate_metrics(y_axis)\n",
        "\n",
        "    # Plot bar graph\n",
        "    plt.bar(y_pos, y_axis, align='center', alpha=0.5, color ='b', yerr=std_dev, tick_label = y_axis)\n",
        "    plt.xticks(y_pos, objects, rotation = 90)\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title('Accuracies over 10 trials')\n",
        "    file_name = 'Mean Accuracy {}.png'.format(mean)\n",
        "    plt.savefig(file_name, bbox_inches='tight')\n",
        "\n",
        "    files.download(file_name)\n",
        "\n",
        "    \n",
        "def plot_line_graph(x_axis, y_axis):\n",
        "    # Specifies the positions on the x_axis for ticks to be labeled\n",
        "    y_pos = np.arange(len(x_axis))\n",
        "\n",
        "    # Formatting for the x_axis labels\n",
        "    objects = ['Iteration {}'.format(i+1) for i in x_axis]\n",
        "\n",
        "    obj = dict(zip(y_pos, y_axis))\n",
        "\n",
        "    # plt.plot('Iterations', 'Accuracy', data=obj)\n",
        "    plt.plot(y_pos, y_axis)\n",
        "    plt.xticks(y_pos, objects)\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title('Accuracies of every model')\n",
        "\n",
        "def calculate_metrics(y_axis):\n",
        "\n",
        "    # Error bar handling\n",
        "    std_dev = np.std(y_axis)\n",
        "    mean = np.mean(y_axis)\n",
        "    variance = np.var(y_axis)\n",
        "\n",
        "    print('Mean accuracy of models are {}'.format(mean))\n",
        "    print('Mean variance of results is {}'.format(variance))\n",
        "    print('Standard deviation of results is {}'.format(std_dev))\n",
        "\n",
        "    return std_dev, mean, variance"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JqEVks-MFrRP"
      },
      "source": [
        "## File read/write\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PogkXQBCI0yo",
        "colab": {}
      },
      "source": [
        "def load_from_csv(path):\n",
        "    '''\n",
        "    Loads the chromosome from path specified into a np array\n",
        "\n",
        "    Args:\n",
        "    path: Path where chromosome is located\n",
        "    e.g path='gdrive/My Drive/Chromosome Saves/chrom_acc72'\n",
        "\n",
        "    Returns:\n",
        "    None\n",
        "    '''\n",
        "    drive.mount('/content/gdrive/')\n",
        "    some_file = np.genfromtxt(path, dtype='float', delimiter=',')\n",
        "\n",
        "    return some_file\n",
        "\n",
        "def update_file_with_metrics(path,iterations, evaluations, time_taken, y_axis, parameter_dict, learning_rate_list):\n",
        "    drive.mount('/content/gdrive/')\n",
        "\n",
        "    std_dev, mean, variance = calculate_metrics(y_axis)\n",
        "    temp_counter = 0\n",
        "    datetime_now = datetime.now() + timedelta(hours=8)\n",
        "\n",
        "    with open(path+'results.txt', 'a+') as f:\n",
        "        f.write('================================\\n')\n",
        "        f.write('New Results\\n')\n",
        "        f.write('================================\\n')\n",
        "        f.write(datetime_now.strftime('%Y-%b-%d, %H%M hours\\n'))\n",
        "        f.write(f'Trials used: {iterations}\\n')\n",
        "        f.write(f'Mean accuracy: {mean}\\n')\n",
        "        f.write(f'Number of evaluations taken: {evaluations}\\n')\n",
        "        f.write(f'Time taken: {time_taken}\\n')\n",
        "        f.write(f'Standard deviation: {std_dev}\\n')\n",
        "        f.write(f'Variance: {variance}\\n')\n",
        "\n",
        "        for key,value in parameter_dict.items():\n",
        "            f.write(f'{key}: {value}\\n')\n",
        "\n",
        "        f.write('List of Accuracies: \\n')\n",
        "        f.write(f'{str(y_axis)}\\n')\n",
        "\n",
        "        f.write(f'List of diff search learning rates: {learning_rate_list}\\n')\n",
        "        for values in learning_rate_list:\n",
        "            temp_counter += values\n",
        "        temp_counter /= iterations\n",
        "        f.write(f'Average learning rate for differential search: {temp_counter}\\n')\n",
        "        f.write('\\n')       \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TRsXu3X1toBF"
      },
      "source": [
        "## Chromosome Evaluation function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CxJe7O5wtrQn",
        "colab": {}
      },
      "source": [
        "# Evaluate a list of chromosomes\n",
        "def eval_chromosomes(list_chromosomes,num_chrom_params):\n",
        "    '''\n",
        "    Evaluates a list of x chromosomes and chooses the best one, where x is number of neighbours specified\n",
        "    \n",
        "    Args:\n",
        "    list_chromosomes: list, containing list of values\n",
        "    num_chrom_params: int, number of parameters a chromosome has\n",
        "\n",
        "    Returns:\n",
        "    best_res: tuple, containg the best results, could be from different chromosomes\n",
        "    temp_list: list, containing dicts of chromosomes\n",
        "    '''\n",
        "\n",
        "    # best_model_accur, best_chromosome, best_model, best_train_params = best_res\n",
        "\n",
        "    best_model_accur = 0\n",
        "    best_chromosome = None\n",
        "    best_model = None\n",
        "    best_train_params = None\n",
        "\n",
        "    num_chrom = len(list_chromosomes)\n",
        "\n",
        "    temp_tuple = ()\n",
        "    temp_list = []\n",
        "\n",
        "    # List of zeroes in the shape [num_chrom, 1+num_chrom_params]\n",
        "    mat_chrom_acur = np.zeros((num_chrom, 1+num_chrom_params))\n",
        "    neighb_valid_accurs = []\n",
        "\n",
        "    for ci, a_chrom in enumerate(list_chromosomes):\n",
        "        # if args['verbose_meta']:\n",
        "        #     print('Chromosome {} ...'.format(ci))\n",
        "\n",
        "        # --- Actual training\n",
        "        model, train_params = prepare_model(a_chrom)\n",
        "        model, valid_accurs, train_accurs = do_eval_chrom(model, train_params, args['num_epochs_search'])\n",
        "        best_valid_accur = max(valid_accurs)\n",
        "\n",
        "        # Store chromosome and accuracy\n",
        "        # mat_chrom_acur will be a (2,376) np array\n",
        "        mat_chrom_acur[ci,0] = best_valid_accur\n",
        "        mat_chrom_acur[ci,1:] = a_chrom\n",
        "        print('Best validation accuracy: {}%.'.format(best_valid_accur))\n",
        "        neighb_valid_accurs.append(best_valid_accur)\n",
        "\n",
        "        if best_valid_accur > best_model_accur:\n",
        "            best_model_accur = best_valid_accur\n",
        "            best_chromosome = a_chrom\n",
        "            best_model = model\n",
        "            best_train_params = train_params\n",
        "\n",
        "        # temp_tuple = (best_valid_accur, a_chrom, model, train_params)\n",
        "\n",
        "        # temp_list.append(temp_tuple)\n",
        "\n",
        "        # if args['verbose_meta']:\n",
        "        #     print('*** Improving validation accuracy: {}.'.format(best_model_accur))\n",
        "\n",
        "    best_res = (best_model_accur, best_chromosome, best_model, best_train_params)\n",
        "    return best_res, mat_chrom_acur\n",
        "\n",
        "# Add early stopping if algorithm does not progress well here\n",
        "def do_eval_chrom(a_model, train_params, num_epochs):\n",
        "    '''\n",
        "    Chromosome evaluation (more efficient than do_training)\n",
        "    Function to train a specific model\n",
        "    Early stopping, or returning best validation model, is not implemented \n",
        "    '''\n",
        "\n",
        "    # Extract basic information\n",
        "    lr1, lr2, lr2_epoch, a_momentum, lr_decr = train_params  \n",
        "    args['lr'] = lr1\n",
        "    args['momentum'] = a_momentum\n",
        "\n",
        "    optimizer = optim.SGD(a_model.parameters(), lr=args['lr'], momentum=args['momentum'])\n",
        "    valid_accurs = []\n",
        "    train_accurs = []\n",
        "    train_start_time = time.time()\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "\n",
        "        if args['verbose_train']:\n",
        "            print('Epoch {} learning rate: {}.'.format(epoch, args['lr']))\n",
        "\n",
        "        train_one_epoch(a_model, optimizer, epoch, train_batches)\n",
        "\n",
        "        # Training accuracy test\n",
        "        a_train_accur = comp_accuracy(a_model, train_batches, num_train_instances)\n",
        "        if args['verbose_train']:\n",
        "            print('Training accuracy: {}%.'.format(a_train_accur))\n",
        "        train_accurs.append(a_train_accur)\n",
        "\n",
        "        # Validation accuracy test\n",
        "        a_valid_accur = comp_accuracy(a_model, valid_batches, num_valid_instances)\n",
        "        if args['verbose_train']:\n",
        "            print('Validation accuracy: {}%.'.format(a_valid_accur))\n",
        "        valid_accurs.append(a_valid_accur)\n",
        "\n",
        "        # Decrement learning rate\n",
        "        if epoch < lr2_epoch:\n",
        "            args['lr'] -= lr_decr\n",
        "            for param_group in optimizer.param_groups:\n",
        "                param_group['lr'] = args['lr']\n",
        "\n",
        "    train_elapsed_time = time.time() - train_start_time\n",
        "    # print('The training process took {} seconds.'.format(train_elapsed_time))\n",
        "\n",
        "    return a_model, valid_accurs, train_accurs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vRtllDvYG9wi"
      },
      "source": [
        "## Architectural Search (Work on this part)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "cellView": "code",
        "id": "IUPxCjD-C9lO",
        "colab": {}
      },
      "source": [
        "# ================================\n",
        "# Architectureal search\n",
        "# ================================\n",
        "\n",
        "# Artificial Intelligence Methods students should focus on the code below.\n",
        "# You should keep the neural network code unchaged. This is crucial for \n",
        "# comparison purposes. In other words, focus only on modifying the architecture\n",
        "# search code below.\n",
        "\n",
        "# --- Architectural search parameters\n",
        "meta = {}\n",
        "meta['max_rs_iter'] = 20 # 10  # initial random search\n",
        "meta['max_shc_iter'] = 15 # 40 # 20 # 40  # stochastic hill climbing iterations\n",
        "meta['num_neighbours'] = 6 # 16 \n",
        "meta['neighbour_range'] = 0.4  # 0.2  # mutation rate for stochastic hill-climbing\n",
        "meta['num_differential_sol'] = 6 # 8 # number of differential evolution solutions\n",
        "meta['diff_lr'] = 0.1 # 0.1 # learning rate for differential search\n",
        "\n",
        "# Start with a small random search\n",
        "print('Initital random search ...')\n",
        "\n",
        "# Performing the search based off the number of iterations defined in settings\n",
        "x_axis_plot = []\n",
        "y_axis_plot = []\n",
        "evaluation_counter = 0\n",
        "\n",
        "# Starting time of 10 iterations\n",
        "total_time_taken = 0\n",
        "iteration_start_time = time.time()\n",
        "\n",
        "for trials in range(10): # 10 iterations\n",
        "    print('======================================')\n",
        "    print('THIS IS TRIAL {}'.format(trials))\n",
        "    print('======================================')\n",
        "\n",
        "    # Can reinitialize with new chromosome\n",
        "    meta_rs_valids = [] # List that will contain best validation accuracies\n",
        "    best_model = None\n",
        "\n",
        "    best_chromosome = None\n",
        "    best_model_accur = 0\n",
        "    population_list = []\n",
        "    # Random search\n",
        "    for rsi in range(meta['max_rs_iter']):\n",
        "        if args['verbose_meta']:\n",
        "            print('Search iteration {}.'.format(rsi+1))\n",
        "\n",
        "        num_chrom_param = comp_num_chrom_param(limits)\n",
        "\n",
        "        # Generate random chromosomes\n",
        "        a_rand_chrom = gen_rand_chromosome(num_chrom_param)\n",
        "\n",
        "        # --- Actual training\n",
        "        # Base model and training parameters\n",
        "        model, train_params = prepare_model(a_rand_chrom)\n",
        "\n",
        "        # New model after evaluation\n",
        "        model, valid_accurs, train_accurs = do_eval_chrom(model, train_params, args['num_epochs_search'])\n",
        "        evaluation_counter += 1\n",
        "        best_valid_accur = max(valid_accurs)\n",
        "        print('Best validation accuracy: {}%.'.format(best_valid_accur))\n",
        "\n",
        "        # Store best model\n",
        "        if best_valid_accur > best_model_accur:\n",
        "            best_model_accur = best_valid_accur\n",
        "            best_model = model\n",
        "            best_train_params = train_params\n",
        "            best_chromosome = a_rand_chrom\n",
        "\n",
        "\n",
        "        population_list.append((best_valid_accur,model, train_params, a_rand_chrom))\n",
        "        \n",
        "        # Best validation accuracy is put into a list\n",
        "        meta_rs_valids.append(best_valid_accur)\n",
        "\n",
        "    print('*****************************************************')\n",
        "    print('Best accuracy after initial random search: {}'.format(best_model_accur))\n",
        "    print('*****************************************************')\n",
        "\n",
        "    if args['verbose_meta']:\n",
        "        print('Best validation errors: {}\\n'.format(meta_rs_valids))\n",
        "\n",
        "    # # --- Stochastic hill climbing\n",
        "    num_chrom_params = best_chromosome.shape[0] # Number of parameters in the chromosome (1st argument of tuple) - 376 params\n",
        "\n",
        "    # Architectural search iterations\n",
        "    best_res = (best_model_accur, best_chromosome, best_model, best_train_params) # Tuple containing best parameters\n",
        "    meta_start_time = time.time()\n",
        "\n",
        "    # Does stochastic hill climbing for ['max_shc_iter'] times\n",
        "    for shci in range(meta['max_shc_iter']):\n",
        "        best_model_accur, best_chromosome, best_model, best_train_params = best_res\n",
        "\n",
        "        if args['verbose_meta']:\n",
        "            print('======================================')\n",
        "            print('Stochastic hill-climbing iteration {}.'.format(shci))\n",
        "            print('======================================')\n",
        "            print('Best accuracy so far: {}'.format(best_model_accur))\n",
        "            print('======================================')\n",
        "\n",
        "        # --- Create a set of stochastic neighbours from the current best model\n",
        "        # Random neighbours\n",
        "        chrom_neighbors = create_neighbours(best_chromosome, meta, num_chrom_params)\n",
        "\n",
        "        # Test validation accuracies of neighbours\n",
        "        new_res, mat_chrom_acur = eval_chromosomes(chrom_neighbors,num_chrom_params)\n",
        "\n",
        "        if new_res[0] > best_res[0]:\n",
        "            best_res = new_res\n",
        "\n",
        "        print('***********************************************')\n",
        "        print('Best accuracy after random mutation: {}'.format(new_res[0]))\n",
        "        print('***********************************************')\n",
        "        \n",
        "        # --- Simple differential search\n",
        "        print('***** Differential Search *********************')\n",
        "        diff_chromosomes = do_diff_chrom_v1(mat_chrom_acur, meta['num_differential_sol'])\n",
        "        new_res, mat_chrom_acur = eval_chromosomes(diff_chromosomes,num_chrom_params)\n",
        "\n",
        "        if new_res[0] > best_res[0]:\n",
        "            best_res = new_res\n",
        "\n",
        "        print('***********************************************')\n",
        "        print('Best accuracy after differential search: {}'.format(new_res[0]))\n",
        "        print('***********************************************')\n",
        "\n",
        "        meta_elapsed_time = time.time() - meta_start_time\n",
        "\n",
        "        if args['verbose_meta']:\n",
        "            print('=====================================================')\n",
        "            print('Architectural optimization total time: {}.'.format(meta_elapsed_time))\n",
        "            print('=====================================================')\n",
        "        \n",
        "        # Final visualization\n",
        "        print('======================================')\n",
        "        print('Best accuracy so far: {}'.format(best_res[0]))\n",
        "        print('======================================')\n",
        "\n",
        "        print('Computing the final test ...')\n",
        "        best_model_accur, best_chromosome, best_model, best_train_params = best_res\n",
        "        final_model, best_valid_model, valid_accurs, train_accurs = do_training(best_model, best_train_params, args['num_epochs_test'])\n",
        "\n",
        "        print('=====================================')\n",
        "        print('Training accuracies')\n",
        "        print('=====================================')\n",
        "        print(train_accurs)\n",
        "\n",
        "        # print('=====================================')\n",
        "        # print('Best training parameters')\n",
        "        # print('=====================================')\n",
        "        # print(best_train_params)\n",
        "\n",
        "        print('=====================================')\n",
        "        # print('Best model')\n",
        "        # print('=====================================')\n",
        "        # print(best_valid_model)\n",
        "\n",
        "        print('=====================================')\n",
        "        print('Test accuracies')\n",
        "        print('=====================================')\n",
        "\n",
        "        print('Model with best validation accuracy: ')\n",
        "        accur_valid = final_test(best_valid_model)\n",
        "\n",
        "        print('Model at the end of training: ')\n",
        "        accur_final = final_test(final_model)\n",
        "\n",
        "        max_accur = int(max(accur_valid, accur_final))\n",
        "\n",
        "        # --- Saving the best chromosome as a csv file\n",
        "        if args['save_best_chrom']:\n",
        "            print('Saving the best chromosome in the root directory (My Drive) ...')\n",
        "            drive.mount('/content/gdrive/')\n",
        "\n",
        "            def get_date_time_str():\n",
        "                now = datetime.now() # current date and time\n",
        "                return now.strftime(\"%d%m%Y_%H%M\")\n",
        "\n",
        "            # Create filename\n",
        "            path = '/content/gdrive/My Drive/Chromosome Saves/Original Code Base chromosomes/'\n",
        "            # filename = 'chrom_acc'+ str(max_accur) +'_iteration' + str(iterations)\n",
        "            filename = 'chrom_' + get_date_time_str() + version + str(max_accur)\n",
        "            savetxt(path+filename, best_chromosome, delimiter=',')\n",
        "            \n",
        "    # Creating a list of values to plot graph / statistical analysis\n",
        "    x_axis_plot.append(iterations)\n",
        "    y_axis_plot.append(max_accur)\n",
        "\n",
        "    total_time_taken = time.time() - iteration_start_time\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kBTp6w0Nzd_X"
      },
      "source": [
        "## Display Graph, Write to File"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vXZ2hP43nq_-",
        "colab": {}
      },
      "source": [
        "# Update file with final metrics\n",
        "path = '/content/gdrive/My Drive/Chromosome Saves/'\n",
        "update_file_with_metrics(path, trials+1, evaluation_counter, total_time_taken, y_axis_plot, meta, diff_lr_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZBmEDFd2ZkT_",
        "colab": {}
      },
      "source": [
        "# Plot bar graph, line graph \n",
        "plot_bar_2(x_axis_plot, y_axis_plot)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FCl2YedlFrAM"
      },
      "source": [
        "## Save chromosome"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OAjk-c31Fs9-",
        "colab": {}
      },
      "source": [
        "# --- Saving the best chromosome as a csv file\n",
        "# if args['save_best_chrom']:\n",
        "#     print('Saving the best chromosome in the root directory (My Drive) ...')\n",
        "#     drive.mount('/content/gdrive/')\n",
        "\n",
        "#     def get_date_time_str():\n",
        "#         now = datetime.now() # current date and time\n",
        "#         return now.strftime(\"%d%m%Y_%H%M\") \n",
        "\n",
        "#     # Create filename\n",
        "#     path = '/content/gdrive/My Drive/Chromosome Saves/'\n",
        "#     filename = 'chrom_acc'+ str(max_accur)\n",
        "# #   filename = 'chrom_' + get_date_time_str() + version + str(max_accur)\n",
        "#     savetxt(path+filename, best_chromosome, delimiter=',')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEZPAf_ozAzQ",
        "colab_type": "text"
      },
      "source": [
        "# Testing Site"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-WkEjTIy__T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# --- Architectural search parameters\n",
        "meta = {}\n",
        "meta['max_rs_iter'] = 20 # 10  # initial random search\n",
        "meta['max_shc_iter'] = 15 # 40 # 20 # 40  # stochastic hill climbing iterations\n",
        "meta['num_neighbours'] = 6 # 16 \n",
        "meta['neighbour_range'] = 0.4  # 0.2  # mutation rate for stochastic hill-climbing\n",
        "meta['num_differential_sol'] = 6 # 8 # number of differential evolution solutions\n",
        "meta['diff_lr'] = 0.1 # 0.1 # learning rate for differential search\n",
        "\n",
        "# Start with a small random search\n",
        "print('Initital random search ...')\n",
        "\n",
        "# Performing the search based off the number of iterations defined in settings\n",
        "x_axis_plot = []\n",
        "y_axis_plot = []\n",
        "evaluation_counter = 0\n",
        "\n",
        "# Starting time of 10 iterations\n",
        "total_time_taken = 0\n",
        "iteration_start_time = time.time()\n",
        "\n",
        "for trials in range(10): # 10 iterations\n",
        "    print('======================================')\n",
        "    print('THIS IS TRIAL {}'.format(trials))\n",
        "    print('======================================')\n",
        "\n",
        "    # Can reinitialize with new chromosome\n",
        "    meta_rs_valids = [] # List that will contain best validation accuracies\n",
        "    best_model = None\n",
        "    best_chromosome = None\n",
        "    best_model_accur = 0\n",
        "    population_list = []\n",
        "\n",
        "    # Random search\n",
        "    # Use this to generate chromosomes\n",
        "    for rsi in range(meta['max_rs_iter']):\n",
        "        if args['verbose_meta']:\n",
        "            print('Search iteration {}.'.format(rsi+1))\n",
        "\n",
        "        num_chrom_param = comp_num_chrom_param(limits)\n",
        "\n",
        "        # Generate random chromosomes\n",
        "        a_rand_chrom = gen_rand_chromosome(num_chrom_param)\n",
        "\n",
        "        # --- Actual training\n",
        "        # Base model and training parameters\n",
        "        model, train_params = prepare_model(a_rand_chrom)\n",
        "\n",
        "        # New model after evaluation\n",
        "        # Evaluation of chromosome\n",
        "        model, valid_accurs, train_accurs = do_eval_chrom(model, train_params, args['num_epochs_search'])\n",
        "        evaluation_counter += 1\n",
        "\n",
        "        best_valid_accur = max(valid_accurs)\n",
        "        print('Best validation accuracy: {}%.\\n'.format(best_valid_accur))\n",
        "\n",
        "        # Append all accuracy, model, train params and chromosome to a list in a tuple\n",
        "        population_list.append((best_valid_accur, model, train_params, a_rand_chrom))\n",
        "        \n",
        "        # Best validation accuracy is put into a list\n",
        "        # meta_rs_valids.append(best_valid_accur)\n",
        "\n",
        "    # print('*****************************************************')\n",
        "    # print('Best accuracy after initial random search: {}'.format(best_model_accur))\n",
        "    # print('*****************************************************')\n",
        "\n",
        "    # if args['verbose_meta']:\n",
        "    #     print('Best validation errors: {}\\n'.format(meta_rs_valids))\n",
        "\n",
        "    # Evaluate the population\n",
        "    # List of tuples in descending order based off first key\n",
        "    population_list.sort(key=lambda tuples:tuples[0], reverse=True)\n",
        "\n",
        "    best_model_accur = population_list[0][0]\n",
        "    best_model = population_list[0][1]\n",
        "    best_train_params = population_list[0][2]\n",
        "    best_chromosome = population_list[0][3]\n",
        "\n",
        "    print(f'Best model acc: {best_model_accur}')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmArIL0M1miu",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QPZwafyWGaY4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "offspring_list = generate_offspring(population_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJt0Ki_zMchs",
        "colab_type": "text"
      },
      "source": [
        "## Generate Offspring"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r838jD_Xanu5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_offspring(population_list, offspring_num=10):\n",
        "\n",
        "    # ========================================================\n",
        "    #                      Reproduction                       \n",
        "    # ========================================================\n",
        "    #     - Performs reproduction to generate offspring\n",
        "    # ========================================================\n",
        "\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        population_list: List of exisitng population\n",
        "        offspring_num: An integer value representing num of offspring\n",
        "                        to be produced, defaulted to 10\n",
        "\n",
        "    Returns:\n",
        "        offspring_list: List of offspring generated by the algorithm\n",
        "    \"\"\"\n",
        "\n",
        "    # Select the first 10 chromosomes (tuples actually)\n",
        "    # Perhaps can use fitness wheel to do it \n",
        "    population_list.sort(key=lambda tuples:tuples[0], reverse=True)\n",
        "\n",
        "    # Select top 10 chrom based off accuracy\n",
        "    mating_pool = population_list[0:10]\n",
        "\n",
        "    # Length of chromosome\n",
        "    chrom_len = len(mating_pool[0][3])\n",
        "    offspring_list = []\n",
        "\n",
        "    # Produces 10 offspring\n",
        "    for i in range(offspring_num):\n",
        "        crossover_point = random.randint(1,chrom_len-1)\n",
        "        rand_chrom1 = random.randint(0,9)\n",
        "        rand_chrom2 = random.randint(0,9)\n",
        "        offspring = []\n",
        "\n",
        "        if rand_chrom1 != rand_chrom2:\n",
        "            parent1 = mating_pool[rand_chrom1][3][0:crossover_point]\n",
        "            parent2 = mating_pool[rand_chrom2][3][crossover_point:]\n",
        "            print(f'Chromosome {rand_chrom1} and {rand_chrom2} were selected to mate')\n",
        "            print(f'Crossover point is {crossover_point}\\n')\n",
        "\n",
        "        else:\n",
        "            print(f'Value of chrom1 and chrom2 are {rand_chrom1} & {rand_chrom2}')\n",
        "            while rand_chrom1 == rand_chrom2:\n",
        "                rand_chrom1 = random.randint(0,9)\n",
        "                rand_chrom2 = random.randint(0,9)\n",
        "                print('Retinitializing rand_chrom\\n')\n",
        "\n",
        "            parent1 = mating_pool[rand_chrom1][3][0:crossover_point]\n",
        "            parent2 = mating_pool[rand_chrom2][3][crossover_point:]\n",
        "            print(f'Chromosome {rand_chrom1} and {rand_chrom2} were selected to mate')\n",
        "            print(f'Crossover point is {crossover_point}\\n')\n",
        "\n",
        "        offspring[0:crossover_point] = parent1\n",
        "        offspring[crossover_point:] = parent2\n",
        "        offspring_list.append(offspring)\n",
        "\n",
        "    # check for length of chromosome\n",
        "    for i in range(len(offspring_list)):\n",
        "        if len(offspring_list[i]) != 376:\n",
        "            print(f'Error, num of chromosomes only at {len(offspring_list[i])}')\n",
        "            break\n",
        "\n",
        "    # Mutation\n",
        "    for offspring in offspring_list:\n",
        "        mutation_prob = np.random.rand()\n",
        "\n",
        "        if mutation_prob < 0.025:\n",
        "            mutated_allele = np.random.randint(0,376,2)\n",
        "            print(f'Mutated allele is {mutated_allele}\\n')\n",
        "            start_mutate = min(mutated_allele)\n",
        "            end_mutate = max(mutated_allele)\n",
        "\n",
        "            mutate_section = offspring[start_mutate:end_mutate]\n",
        "            print(f'Section before mutate is {offspring[start_mutate:end_mutate]}')\n",
        "            np.random.shuffle(mutate_section)\n",
        "            offspring[start_mutate:end_mutate] = mutate_section\n",
        "            print(f'Section after mutate is {offspring[start_mutate:end_mutate]}')\n",
        "\n",
        "        print(f'mutation prob: {mutation_prob}')\n",
        "\n",
        "    # print(f'Number of offsprings generated are {len(offspring_list)}')\n",
        "    # print(f'Number of chromosomes in offspring are {len(offspring_list[0])}')\n",
        "\n",
        "    return offspring_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BVJFueB9sdEN",
        "colab_type": "code",
        "outputId": "5106a189-11de-4ea3-d8b0-8c50a5eadc10",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# # --- Stochastic hill climbing\n",
        "    num_chrom_params = best_chromosome.shape[0] # Number of parameters in the chromosome (1st argument of tuple) - 376 params\n",
        "\n",
        "    # Architectural search iterations\n",
        "    best_res = (best_model_accur, best_chromosome, best_model, best_train_params) # Tuple containing best parameters\n",
        "    meta_start_time = time.time()\n",
        "\n",
        "    # Does stochastic hill climbing for ['max_shc_iter'] times\n",
        "    for shci in range(meta['max_shc_iter']):\n",
        "        best_model_accur, best_chromosome, best_model, best_train_params = best_res\n",
        "\n",
        "        if args['verbose_meta']:\n",
        "            print('======================================')\n",
        "            print('Stochastic hill-climbing iteration {}.'.format(shci))\n",
        "            print('======================================')\n",
        "            print('Best accuracy so far: {}'.format(best_model_accur))\n",
        "            print('======================================')\n",
        "\n",
        "        # --- Create a set of stochastic neighbours from the current best model\n",
        "        # Random neighbours\n",
        "        chrom_neighbors = create_neighbours(best_chromosome, meta, num_chrom_params)\n",
        "\n",
        "        # Test validation accuracies of neighbours\n",
        "        new_res, mat_chrom_acur = eval_chromosomes(chrom_neighbors,num_chrom_params)\n",
        "\n",
        "        if new_res[0] > best_res[0]:\n",
        "            best_res = new_res\n",
        "\n",
        "        print('***********************************************')\n",
        "        print('Best accuracy after random mutation: {}'.format(new_res[0]))\n",
        "        print('***********************************************')\n",
        "        \n",
        "        # --- Simple differential search\n",
        "        print('***** Differential Search *********************')\n",
        "        diff_chromosomes = do_diff_chrom_v1(mat_chrom_acur, meta['num_differential_sol'])\n",
        "        new_res, mat_chrom_acur = eval_chromosomes(diff_chromosomes,num_chrom_params)\n",
        "\n",
        "        if new_res[0] > best_res[0]:\n",
        "            best_res = new_res\n",
        "\n",
        "        print('***********************************************')\n",
        "        print('Best accuracy after differential search: {}'.format(new_res[0]))\n",
        "        print('***********************************************')\n",
        "\n",
        "        meta_elapsed_time = time.time() - meta_start_time\n",
        "\n",
        "        if args['verbose_meta']:\n",
        "            print('=====================================================')\n",
        "            print('Architectural optimization total time: {}.'.format(meta_elapsed_time))\n",
        "            print('=====================================================')\n",
        "        \n",
        "        # Final visualization\n",
        "        print('======================================')\n",
        "        print('Best accuracy so far: {}'.format(best_res[0]))\n",
        "        print('======================================')\n",
        "\n",
        "        print('Computing the final test ...')\n",
        "        best_model_accur, best_chromosome, best_model, best_train_params = best_res\n",
        "        final_model, best_valid_model, valid_accurs, train_accurs = do_training(best_model, best_train_params, args['num_epochs_test'])\n",
        "\n",
        "        print('=====================================')\n",
        "        print('Training accuracies')\n",
        "        print('=====================================')\n",
        "        print(train_accurs)\n",
        "\n",
        "        # print('=====================================')\n",
        "        # print('Best training parameters')\n",
        "        # print('=====================================')\n",
        "        # print(best_train_params)\n",
        "\n",
        "        print('=====================================')\n",
        "        # print('Best model')\n",
        "        # print('=====================================')\n",
        "        # print(best_valid_model)\n",
        "\n",
        "        print('=====================================')\n",
        "        print('Test accuracies')\n",
        "        print('=====================================')\n",
        "\n",
        "        print('Model with best validation accuracy: ')\n",
        "        accur_valid = final_test(best_valid_model)\n",
        "\n",
        "        print('Model at the end of training: ')\n",
        "        accur_final = final_test(final_model)\n",
        "\n",
        "        max_accur = int(max(accur_valid, accur_final))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "======================================\n",
            "Stochastic hill-climbing iteration 0.\n",
            "======================================\n",
            "Best accuracy so far: 70.8\n",
            "======================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:100: UserWarning: Implicit dimension choice for softmin has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Best validation accuracy: 9.8%.\n",
            "Best validation accuracy: 10.9%.\n",
            "Best validation accuracy: 10.7%.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:100: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Best validation accuracy: 10.7%.\n",
            "Best validation accuracy: 10.7%.\n",
            "Best validation accuracy: 9.8%.\n",
            "***********************************************\n",
            "Best accuracy after random mutation: 10.9\n",
            "***********************************************\n",
            "***** Differential Search *********************\n",
            "Best validation accuracy: 10.9%.\n",
            "Best validation accuracy: 10.9%.\n",
            "Best validation accuracy: 10.9%.\n",
            "Best validation accuracy: 10.9%.\n",
            "Best validation accuracy: 10.7%.\n",
            "***********************************************\n",
            "Best accuracy after differential search: 10.9\n",
            "***********************************************\n",
            "=====================================================\n",
            "Architectural optimization total time: 3.288461208343506.\n",
            "=====================================================\n",
            "======================================\n",
            "Best accuracy so far: 70.8\n",
            "======================================\n",
            "Computing the final test ...\n",
            "=====================================\n",
            "Training accuracies\n",
            "=====================================\n",
            "[84.0, 94.0, 87.0, 68.0, 56.99999999999999, 45.0, 41.0, 38.0, 37.0, 35.0, 32.0, 31.0, 31.0, 63.0, 95.0, 84.0, 90.0, 100.0, 97.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.0, 100.0, 100.0, 99.0, 100.0, 100.0, 98.0, 97.0, 99.0, 98.0, 92.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.0, 100.0, 100.0, 100.0, 94.0, 100.0, 100.0, 100.0, 100.0, 99.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.0, 99.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.0, 99.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 98.0, 97.0, 100.0, 98.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.0, 99.0, 99.0, 99.0, 99.0, 99.0, 99.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]\n",
            "=====================================\n",
            "=====================================\n",
            "Test accuracies\n",
            "=====================================\n",
            "Model with best validation accuracy: \n",
            "\n",
            "Test set: Average loss: 1.4578, Accuracy (at final epoch): 7232/10000 (72%)\n",
            "\n",
            "Model at the end of training: \n",
            "\n",
            "Test set: Average loss: 1.6489, Accuracy (at final epoch): 7004/10000 (70%)\n",
            "\n",
            "======================================\n",
            "Stochastic hill-climbing iteration 1.\n",
            "======================================\n",
            "Best accuracy so far: 70.8\n",
            "======================================\n",
            "Best validation accuracy: 10.7%.\n",
            "Best validation accuracy: 9.8%.\n",
            "Best validation accuracy: 10.7%.\n",
            "Best validation accuracy: 10.0%.\n",
            "Best validation accuracy: 10.7%.\n",
            "Best validation accuracy: 10.9%.\n",
            "***********************************************\n",
            "Best accuracy after random mutation: 10.9\n",
            "***********************************************\n",
            "***** Differential Search *********************\n",
            "Best validation accuracy: 10.0%.\n",
            "Best validation accuracy: 10.9%.\n",
            "Best validation accuracy: 10.0%.\n",
            "Best validation accuracy: 10.9%.\n",
            "Best validation accuracy: 10.0%.\n",
            "***********************************************\n",
            "Best accuracy after differential search: 10.9\n",
            "***********************************************\n",
            "=====================================================\n",
            "Architectural optimization total time: 44.45758247375488.\n",
            "=====================================================\n",
            "======================================\n",
            "Best accuracy so far: 70.8\n",
            "======================================\n",
            "Computing the final test ...\n",
            "=====================================\n",
            "Training accuracies\n",
            "=====================================\n",
            "[100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.0, 99.0, 99.0, 99.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]\n",
            "=====================================\n",
            "=====================================\n",
            "Test accuracies\n",
            "=====================================\n",
            "Model with best validation accuracy: \n",
            "\n",
            "Test set: Average loss: 1.6794, Accuracy (at final epoch): 7205/10000 (72%)\n",
            "\n",
            "Model at the end of training: \n",
            "\n",
            "Test set: Average loss: 2.3106, Accuracy (at final epoch): 7204/10000 (72%)\n",
            "\n",
            "======================================\n",
            "Stochastic hill-climbing iteration 2.\n",
            "======================================\n",
            "Best accuracy so far: 70.8\n",
            "======================================\n",
            "Best validation accuracy: 10.7%.\n",
            "Best validation accuracy: 10.7%.\n",
            "Best validation accuracy: 9.8%.\n",
            "Best validation accuracy: 10.7%.\n",
            "Best validation accuracy: 9.8%.\n",
            "Best validation accuracy: 21.7%.\n",
            "***********************************************\n",
            "Best accuracy after random mutation: 21.7\n",
            "***********************************************\n",
            "***** Differential Search *********************\n",
            "Best validation accuracy: 20.599999999999998%.\n",
            "Best validation accuracy: 10.7%.\n",
            "Best validation accuracy: 63.1%.\n",
            "Best validation accuracy: 15.4%.\n",
            "Best validation accuracy: 26.200000000000003%.\n",
            "***********************************************\n",
            "Best accuracy after differential search: 63.1\n",
            "***********************************************\n",
            "=====================================================\n",
            "Architectural optimization total time: 84.50121927261353.\n",
            "=====================================================\n",
            "======================================\n",
            "Best accuracy so far: 70.8\n",
            "======================================\n",
            "Computing the final test ...\n",
            "=====================================\n",
            "Training accuracies\n",
            "=====================================\n",
            "[100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]\n",
            "=====================================\n",
            "=====================================\n",
            "Test accuracies\n",
            "=====================================\n",
            "Model with best validation accuracy: \n",
            "\n",
            "Test set: Average loss: 2.1244, Accuracy (at final epoch): 7257/10000 (73%)\n",
            "\n",
            "Model at the end of training: \n",
            "\n",
            "Test set: Average loss: 2.2244, Accuracy (at final epoch): 7193/10000 (72%)\n",
            "\n",
            "======================================\n",
            "Stochastic hill-climbing iteration 3.\n",
            "======================================\n",
            "Best accuracy so far: 70.8\n",
            "======================================\n",
            "Best validation accuracy: 10.0%.\n",
            "Best validation accuracy: 9.8%.\n",
            "Best validation accuracy: 9.8%.\n",
            "Best validation accuracy: 10.9%.\n",
            "Best validation accuracy: 10.9%.\n",
            "Best validation accuracy: 10.9%.\n",
            "***********************************************\n",
            "Best accuracy after random mutation: 10.9\n",
            "***********************************************\n",
            "***** Differential Search *********************\n",
            "Best validation accuracy: 10.7%.\n",
            "Best validation accuracy: 10.7%.\n",
            "Best validation accuracy: 10.9%.\n",
            "Best validation accuracy: 10.7%.\n",
            "Best validation accuracy: 10.9%.\n",
            "***********************************************\n",
            "Best accuracy after differential search: 10.9\n",
            "***********************************************\n",
            "=====================================================\n",
            "Architectural optimization total time: 124.19571566581726.\n",
            "=====================================================\n",
            "======================================\n",
            "Best accuracy so far: 70.8\n",
            "======================================\n",
            "Computing the final test ...\n",
            "=====================================\n",
            "Training accuracies\n",
            "=====================================\n",
            "[100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]\n",
            "=====================================\n",
            "=====================================\n",
            "Test accuracies\n",
            "=====================================\n",
            "Model with best validation accuracy: \n",
            "\n",
            "Test set: Average loss: 2.1485, Accuracy (at final epoch): 7249/10000 (72%)\n",
            "\n",
            "Model at the end of training: \n",
            "\n",
            "Test set: Average loss: 2.0631, Accuracy (at final epoch): 7236/10000 (72%)\n",
            "\n",
            "======================================\n",
            "Stochastic hill-climbing iteration 4.\n",
            "======================================\n",
            "Best accuracy so far: 70.8\n",
            "======================================\n",
            "Best validation accuracy: 12.1%.\n",
            "Best validation accuracy: 9.8%.\n",
            "Best validation accuracy: 9.8%.\n",
            "Best validation accuracy: 10.9%.\n",
            "Best validation accuracy: 10.9%.\n",
            "Best validation accuracy: 9.8%.\n",
            "***********************************************\n",
            "Best accuracy after random mutation: 12.1\n",
            "***********************************************\n",
            "***** Differential Search *********************\n",
            "Best validation accuracy: 10.9%.\n",
            "Best validation accuracy: 10.0%.\n",
            "Best validation accuracy: 10.9%.\n",
            "Best validation accuracy: 10.7%.\n",
            "Best validation accuracy: 18.099999999999998%.\n",
            "***********************************************\n",
            "Best accuracy after differential search: 18.099999999999998\n",
            "***********************************************\n",
            "=====================================================\n",
            "Architectural optimization total time: 164.2585711479187.\n",
            "=====================================================\n",
            "======================================\n",
            "Best accuracy so far: 70.8\n",
            "======================================\n",
            "Computing the final test ...\n",
            "=====================================\n",
            "Training accuracies\n",
            "=====================================\n",
            "[100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]\n",
            "=====================================\n",
            "=====================================\n",
            "Test accuracies\n",
            "=====================================\n",
            "Model with best validation accuracy: \n",
            "\n",
            "Test set: Average loss: 1.7495, Accuracy (at final epoch): 7387/10000 (74%)\n",
            "\n",
            "Model at the end of training: \n",
            "\n",
            "Test set: Average loss: 2.0029, Accuracy (at final epoch): 7320/10000 (73%)\n",
            "\n",
            "======================================\n",
            "Stochastic hill-climbing iteration 5.\n",
            "======================================\n",
            "Best accuracy so far: 70.8\n",
            "======================================\n",
            "Best validation accuracy: 10.7%.\n",
            "Best validation accuracy: 10.7%.\n",
            "Best validation accuracy: 10.7%.\n",
            "Best validation accuracy: 10.9%.\n",
            "Best validation accuracy: 10.9%.\n",
            "Best validation accuracy: 10.0%.\n",
            "***********************************************\n",
            "Best accuracy after random mutation: 10.9\n",
            "***********************************************\n",
            "***** Differential Search *********************\n",
            "Best validation accuracy: 10.9%.\n",
            "Best validation accuracy: 10.9%.\n",
            "Best validation accuracy: 10.9%.\n",
            "Best validation accuracy: 10.7%.\n",
            "Best validation accuracy: 10.9%.\n",
            "***********************************************\n",
            "Best accuracy after differential search: 10.9\n",
            "***********************************************\n",
            "=====================================================\n",
            "Architectural optimization total time: 204.13450455665588.\n",
            "=====================================================\n",
            "======================================\n",
            "Best accuracy so far: 70.8\n",
            "======================================\n",
            "Computing the final test ...\n",
            "=====================================\n",
            "Training accuracies\n",
            "=====================================\n",
            "[100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 93.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]\n",
            "=====================================\n",
            "=====================================\n",
            "Test accuracies\n",
            "=====================================\n",
            "Model with best validation accuracy: \n",
            "\n",
            "Test set: Average loss: 1.6505, Accuracy (at final epoch): 7430/10000 (74%)\n",
            "\n",
            "Model at the end of training: \n",
            "\n",
            "Test set: Average loss: 1.6899, Accuracy (at final epoch): 7394/10000 (74%)\n",
            "\n",
            "======================================\n",
            "Stochastic hill-climbing iteration 6.\n",
            "======================================\n",
            "Best accuracy so far: 70.8\n",
            "======================================\n",
            "Best validation accuracy: 10.7%.\n",
            "Best validation accuracy: 10.7%.\n",
            "Best validation accuracy: 10.7%.\n",
            "Best validation accuracy: 10.9%.\n",
            "Best validation accuracy: 10.9%.\n",
            "Best validation accuracy: 10.7%.\n",
            "***********************************************\n",
            "Best accuracy after random mutation: 10.9\n",
            "***********************************************\n",
            "***** Differential Search *********************\n",
            "Best validation accuracy: 9.8%.\n",
            "Best validation accuracy: 9.8%.\n",
            "Best validation accuracy: 9.8%.\n",
            "Best validation accuracy: 9.8%.\n",
            "Best validation accuracy: 9.8%.\n",
            "***********************************************\n",
            "Best accuracy after differential search: 9.8\n",
            "***********************************************\n",
            "=====================================================\n",
            "Architectural optimization total time: 243.9447124004364.\n",
            "=====================================================\n",
            "======================================\n",
            "Best accuracy so far: 70.8\n",
            "======================================\n",
            "Computing the final test ...\n",
            "=====================================\n",
            "Training accuracies\n",
            "=====================================\n",
            "[100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]\n",
            "=====================================\n",
            "=====================================\n",
            "Test accuracies\n",
            "=====================================\n",
            "Model with best validation accuracy: \n",
            "\n",
            "Test set: Average loss: 1.5828, Accuracy (at final epoch): 7459/10000 (75%)\n",
            "\n",
            "Model at the end of training: \n",
            "\n",
            "Test set: Average loss: 1.6327, Accuracy (at final epoch): 7400/10000 (74%)\n",
            "\n",
            "======================================\n",
            "Stochastic hill-climbing iteration 7.\n",
            "======================================\n",
            "Best accuracy so far: 70.8\n",
            "======================================\n",
            "Best validation accuracy: 10.9%.\n",
            "Best validation accuracy: 10.0%.\n",
            "Best validation accuracy: 10.7%.\n",
            "Best validation accuracy: 10.9%.\n",
            "Best validation accuracy: 9.8%.\n",
            "Best validation accuracy: 10.7%.\n",
            "***********************************************\n",
            "Best accuracy after random mutation: 10.9\n",
            "***********************************************\n",
            "***** Differential Search *********************\n",
            "Best validation accuracy: 10.7%.\n",
            "Best validation accuracy: 10.7%.\n",
            "Best validation accuracy: 10.7%.\n",
            "Best validation accuracy: 10.0%.\n",
            "Best validation accuracy: 10.7%.\n",
            "***********************************************\n",
            "Best accuracy after differential search: 10.7\n",
            "***********************************************\n",
            "=====================================================\n",
            "Architectural optimization total time: 284.53431701660156.\n",
            "=====================================================\n",
            "======================================\n",
            "Best accuracy so far: 70.8\n",
            "======================================\n",
            "Computing the final test ...\n",
            "=====================================\n",
            "Training accuracies\n",
            "=====================================\n",
            "[100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]\n",
            "=====================================\n",
            "=====================================\n",
            "Test accuracies\n",
            "=====================================\n",
            "Model with best validation accuracy: \n",
            "\n",
            "Test set: Average loss: 1.6684, Accuracy (at final epoch): 7452/10000 (75%)\n",
            "\n",
            "Model at the end of training: \n",
            "\n",
            "Test set: Average loss: 1.8380, Accuracy (at final epoch): 7366/10000 (74%)\n",
            "\n",
            "======================================\n",
            "Stochastic hill-climbing iteration 8.\n",
            "======================================\n",
            "Best accuracy so far: 70.8\n",
            "======================================\n",
            "Best validation accuracy: 10.9%.\n",
            "Best validation accuracy: 10.9%.\n",
            "Best validation accuracy: 10.9%.\n",
            "Best validation accuracy: 9.8%.\n",
            "Best validation accuracy: 10.7%.\n",
            "Best validation accuracy: 13.900000000000002%.\n",
            "***********************************************\n",
            "Best accuracy after random mutation: 13.900000000000002\n",
            "***********************************************\n",
            "***** Differential Search *********************\n",
            "Best validation accuracy: 10.0%.\n",
            "Best validation accuracy: 10.9%.\n",
            "Best validation accuracy: 13.100000000000001%.\n",
            "Best validation accuracy: 10.0%.\n",
            "Best validation accuracy: 10.9%.\n",
            "***********************************************\n",
            "Best accuracy after differential search: 13.100000000000001\n",
            "***********************************************\n",
            "=====================================================\n",
            "Architectural optimization total time: 324.4275281429291.\n",
            "=====================================================\n",
            "======================================\n",
            "Best accuracy so far: 70.8\n",
            "======================================\n",
            "Computing the final test ...\n",
            "=====================================\n",
            "Training accuracies\n",
            "=====================================\n",
            "[100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]\n",
            "=====================================\n",
            "=====================================\n",
            "Test accuracies\n",
            "=====================================\n",
            "Model with best validation accuracy: \n",
            "\n",
            "Test set: Average loss: 1.7532, Accuracy (at final epoch): 7429/10000 (74%)\n",
            "\n",
            "Model at the end of training: \n",
            "\n",
            "Test set: Average loss: 1.7737, Accuracy (at final epoch): 7387/10000 (74%)\n",
            "\n",
            "======================================\n",
            "Stochastic hill-climbing iteration 9.\n",
            "======================================\n",
            "Best accuracy so far: 70.8\n",
            "======================================\n",
            "Best validation accuracy: 10.7%.\n",
            "Best validation accuracy: 9.8%.\n",
            "Best validation accuracy: 9.8%.\n",
            "Best validation accuracy: 9.8%.\n",
            "Best validation accuracy: 10.9%.\n",
            "Best validation accuracy: 9.8%.\n",
            "***********************************************\n",
            "Best accuracy after random mutation: 10.9\n",
            "***********************************************\n",
            "***** Differential Search *********************\n",
            "Best validation accuracy: 10.9%.\n",
            "Best validation accuracy: 10.7%.\n",
            "Best validation accuracy: 9.8%.\n",
            "Best validation accuracy: 9.8%.\n",
            "Best validation accuracy: 10.9%.\n",
            "***********************************************\n",
            "Best accuracy after differential search: 10.9\n",
            "***********************************************\n",
            "=====================================================\n",
            "Architectural optimization total time: 364.60509967803955.\n",
            "=====================================================\n",
            "======================================\n",
            "Best accuracy so far: 70.8\n",
            "======================================\n",
            "Computing the final test ...\n",
            "=====================================\n",
            "Training accuracies\n",
            "=====================================\n",
            "[100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]\n",
            "=====================================\n",
            "=====================================\n",
            "Test accuracies\n",
            "=====================================\n",
            "Model with best validation accuracy: \n",
            "\n",
            "Test set: Average loss: 1.7071, Accuracy (at final epoch): 7386/10000 (74%)\n",
            "\n",
            "Model at the end of training: \n",
            "\n",
            "Test set: Average loss: 1.6964, Accuracy (at final epoch): 7365/10000 (74%)\n",
            "\n",
            "======================================\n",
            "Stochastic hill-climbing iteration 10.\n",
            "======================================\n",
            "Best accuracy so far: 70.8\n",
            "======================================\n",
            "Best validation accuracy: 10.7%.\n",
            "Best validation accuracy: 10.7%.\n",
            "Best validation accuracy: 10.9%.\n",
            "Best validation accuracy: 10.7%.\n",
            "Best validation accuracy: 10.9%.\n",
            "Best validation accuracy: 10.0%.\n",
            "***********************************************\n",
            "Best accuracy after random mutation: 10.9\n",
            "***********************************************\n",
            "***** Differential Search *********************\n",
            "Best validation accuracy: 10.9%.\n",
            "Best validation accuracy: 10.7%.\n",
            "Best validation accuracy: 10.7%.\n",
            "Best validation accuracy: 10.7%.\n",
            "Best validation accuracy: 10.7%.\n",
            "***********************************************\n",
            "Best accuracy after differential search: 10.9\n",
            "***********************************************\n",
            "=====================================================\n",
            "Architectural optimization total time: 404.8331985473633.\n",
            "=====================================================\n",
            "======================================\n",
            "Best accuracy so far: 70.8\n",
            "======================================\n",
            "Computing the final test ...\n",
            "=====================================\n",
            "Training accuracies\n",
            "=====================================\n",
            "[100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]\n",
            "=====================================\n",
            "=====================================\n",
            "Test accuracies\n",
            "=====================================\n",
            "Model with best validation accuracy: \n",
            "\n",
            "Test set: Average loss: 1.6751, Accuracy (at final epoch): 7403/10000 (74%)\n",
            "\n",
            "Model at the end of training: \n",
            "\n",
            "Test set: Average loss: 1.7844, Accuracy (at final epoch): 7354/10000 (74%)\n",
            "\n",
            "======================================\n",
            "Stochastic hill-climbing iteration 11.\n",
            "======================================\n",
            "Best accuracy so far: 70.8\n",
            "======================================\n",
            "Best validation accuracy: 10.0%.\n",
            "Best validation accuracy: 10.9%.\n",
            "Best validation accuracy: 10.9%.\n",
            "Best validation accuracy: 10.7%.\n",
            "Best validation accuracy: 10.7%.\n",
            "Best validation accuracy: 9.8%.\n",
            "***********************************************\n",
            "Best accuracy after random mutation: 10.9\n",
            "***********************************************\n",
            "***** Differential Search *********************\n",
            "Best validation accuracy: 10.0%.\n",
            "Best validation accuracy: 10.7%.\n",
            "Best validation accuracy: 10.9%.\n",
            "Best validation accuracy: 10.9%.\n",
            "Best validation accuracy: 10.9%.\n",
            "***********************************************\n",
            "Best accuracy after differential search: 10.9\n",
            "***********************************************\n",
            "=====================================================\n",
            "Architectural optimization total time: 444.9090781211853.\n",
            "=====================================================\n",
            "======================================\n",
            "Best accuracy so far: 70.8\n",
            "======================================\n",
            "Computing the final test ...\n",
            "=====================================\n",
            "Training accuracies\n",
            "=====================================\n",
            "[100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]\n",
            "=====================================\n",
            "=====================================\n",
            "Test accuracies\n",
            "=====================================\n",
            "Model with best validation accuracy: \n",
            "\n",
            "Test set: Average loss: 1.6662, Accuracy (at final epoch): 7386/10000 (74%)\n",
            "\n",
            "Model at the end of training: \n",
            "\n",
            "Test set: Average loss: 1.8655, Accuracy (at final epoch): 7315/10000 (73%)\n",
            "\n",
            "======================================\n",
            "Stochastic hill-climbing iteration 12.\n",
            "======================================\n",
            "Best accuracy so far: 70.8\n",
            "======================================\n",
            "Best validation accuracy: 9.8%.\n",
            "Best validation accuracy: 9.8%.\n",
            "Best validation accuracy: 10.7%.\n",
            "Best validation accuracy: 9.8%.\n",
            "Best validation accuracy: 10.0%.\n",
            "Best validation accuracy: 10.7%.\n",
            "***********************************************\n",
            "Best accuracy after random mutation: 10.7\n",
            "***********************************************\n",
            "***** Differential Search *********************\n",
            "Best validation accuracy: 10.7%.\n",
            "Best validation accuracy: 10.7%.\n",
            "Best validation accuracy: 10.7%.\n",
            "Best validation accuracy: 10.7%.\n",
            "Best validation accuracy: 10.7%.\n",
            "***********************************************\n",
            "Best accuracy after differential search: 10.7\n",
            "***********************************************\n",
            "=====================================================\n",
            "Architectural optimization total time: 485.0538830757141.\n",
            "=====================================================\n",
            "======================================\n",
            "Best accuracy so far: 70.8\n",
            "======================================\n",
            "Computing the final test ...\n",
            "=====================================\n",
            "Training accuracies\n",
            "=====================================\n",
            "[100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]\n",
            "=====================================\n",
            "=====================================\n",
            "Test accuracies\n",
            "=====================================\n",
            "Model with best validation accuracy: \n",
            "\n",
            "Test set: Average loss: 1.5807, Accuracy (at final epoch): 7415/10000 (74%)\n",
            "\n",
            "Model at the end of training: \n",
            "\n",
            "Test set: Average loss: 1.8128, Accuracy (at final epoch): 7361/10000 (74%)\n",
            "\n",
            "======================================\n",
            "Stochastic hill-climbing iteration 13.\n",
            "======================================\n",
            "Best accuracy so far: 70.8\n",
            "======================================\n",
            "Best validation accuracy: 10.9%.\n",
            "Best validation accuracy: 10.7%.\n",
            "Best validation accuracy: 10.9%.\n",
            "Best validation accuracy: 10.9%.\n",
            "Best validation accuracy: 10.0%.\n",
            "Best validation accuracy: 10.0%.\n",
            "***********************************************\n",
            "Best accuracy after random mutation: 10.9\n",
            "***********************************************\n",
            "***** Differential Search *********************\n",
            "Best validation accuracy: 10.9%.\n",
            "Best validation accuracy: 13.900000000000002%.\n",
            "Best validation accuracy: 20.200000000000003%.\n",
            "Best validation accuracy: 10.7%.\n",
            "Best validation accuracy: 10.9%.\n",
            "***********************************************\n",
            "Best accuracy after differential search: 20.200000000000003\n",
            "***********************************************\n",
            "=====================================================\n",
            "Architectural optimization total time: 524.8468358516693.\n",
            "=====================================================\n",
            "======================================\n",
            "Best accuracy so far: 70.8\n",
            "======================================\n",
            "Computing the final test ...\n",
            "=====================================\n",
            "Training accuracies\n",
            "=====================================\n",
            "[100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]\n",
            "=====================================\n",
            "=====================================\n",
            "Test accuracies\n",
            "=====================================\n",
            "Model with best validation accuracy: \n",
            "\n",
            "Test set: Average loss: 1.6855, Accuracy (at final epoch): 7428/10000 (74%)\n",
            "\n",
            "Model at the end of training: \n",
            "\n",
            "Test set: Average loss: 1.7790, Accuracy (at final epoch): 7383/10000 (74%)\n",
            "\n",
            "======================================\n",
            "Stochastic hill-climbing iteration 14.\n",
            "======================================\n",
            "Best accuracy so far: 70.8\n",
            "======================================\n",
            "Best validation accuracy: 10.7%.\n",
            "Best validation accuracy: 9.8%.\n",
            "Best validation accuracy: 9.8%.\n",
            "Best validation accuracy: 10.0%.\n",
            "Best validation accuracy: 10.0%.\n",
            "Best validation accuracy: 10.7%.\n",
            "***********************************************\n",
            "Best accuracy after random mutation: 10.7\n",
            "***********************************************\n",
            "***** Differential Search *********************\n",
            "Best validation accuracy: 10.9%.\n",
            "Best validation accuracy: 10.7%.\n",
            "Best validation accuracy: 10.7%.\n",
            "Best validation accuracy: 10.9%.\n",
            "Best validation accuracy: 10.9%.\n",
            "***********************************************\n",
            "Best accuracy after differential search: 10.9\n",
            "***********************************************\n",
            "=====================================================\n",
            "Architectural optimization total time: 565.2254073619843.\n",
            "=====================================================\n",
            "======================================\n",
            "Best accuracy so far: 70.8\n",
            "======================================\n",
            "Computing the final test ...\n",
            "=====================================\n",
            "Training accuracies\n",
            "=====================================\n",
            "[100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]\n",
            "=====================================\n",
            "=====================================\n",
            "Test accuracies\n",
            "=====================================\n",
            "Model with best validation accuracy: \n",
            "\n",
            "Test set: Average loss: 1.6793, Accuracy (at final epoch): 7425/10000 (74%)\n",
            "\n",
            "Model at the end of training: \n",
            "\n",
            "Test set: Average loss: 1.8889, Accuracy (at final epoch): 7371/10000 (74%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}